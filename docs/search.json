[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Kailyn Hogan is from Delhi, Iowa, and is a senior at Iowa State University majoring in Community and Regional Planning with a minor in Geographic Information Systems. She is also Vice President of the undergraduate Community and Regional Planning Club at Iowa State. Kailyn believes combining data science and urban planning is vital in today’s data-driven world. After college, Kailyn hopes to join the Peace Corps to continue aiding communities and diversifying her cultural education.\nThis blog outlines her time and experience in the Data Science for the Public Good program through Iowa State University Extension and Outreach. During DSPG, she was a part of the housing group charged with creating an AI Model to objectively assess housing values in rural communities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kailyn Hogan’s DSPG Blog",
    "section": "",
    "text": "Team Blog: Week Seven\n\n\nFinal Presentation Outline\n\n\n\n\nWeek Seven\n\n\nTeam Blog\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Seven of Data Science for the Public Good\n\n\nDemographic Analysis Time\n\n\n\n\nWeek Seven\n\n\nR\n\n\nDemographic Analysis\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Six of Data Science for the Public Good\n\n\nCreating a Demographic Profile\n\n\n\n\nWeek Six\n\n\nR\n\n\nDemographic Analysis\n\n\nWINVEST\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Five of Data Science for the Public Good\n\n\n\n\n\n\n\nWeek Five\n\n\nITAG Conference\n\n\nArcGIS\n\n\nDemographic Analysis\n\n\nAI Models\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nTeam Blog: Week Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four of Data Science for the Public Good\n\n\nGathering addresses and images for our AI Models!\n\n\n\n\nWeek Four\n\n\nExcel\n\n\nWeb Scraping\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three of Data Science for the Public Good\n\n\nI gave a Morning Coffee Talk!\n\n\n\n\nWeek Three\n\n\nAI Models\n\n\nDataCamp\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two of Data Science for the Public Good\n\n\nRelearning TidyCensus in Vegas!\n\n\n\n\nWeek Two\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek One of Data Science for the Public Good\n\n\nDataCamp! DataCamp! DataCamp!\n\n\n\n\nWeek One\n\n\nDataCamp\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#project-overview",
    "href": "posts/Team_Blog_Week4/Team Blog.html#project-overview",
    "title": "Team Blog: Week Four",
    "section": "Project Overview",
    "text": "Project Overview\nThis is the project plan we came up with the first week of DSPG. This project is intended to span over three years with DPSG, and different interns will be working on it in the coming years. Thus, the project plan is ambitious for this summer."
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#problem-statement",
    "href": "posts/Team_Blog_Week4/Team Blog.html#problem-statement",
    "title": "Team Blog: Week Four",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe absence of a comprehensive and unbiased assessment of housing quality in rural communities poses challenges in identifying financing gaps and effectively allocating resources for housing improvement. Consequently, this hinders the overall well-being and health of residents, impacts workforce stability, diminishes rural vitality, and undermines the economic growth of Iowa. Moreover, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations further compound the problem. To address these challenges, there is a pressing need for an AI-driven approach that can provide a more accurate and objective evaluation of housing quality, identify financing gaps, and optimize the allocation of local, state, and federal funds to maximize community benefits.\nUtilizing web scraping techniques to collect images of houses from various assessor websites, an AI model can be developed to analyze and categorize housing features into good or poor quality. This can enable targeted investment strategies. It allows for the identification of houses in need of improvement and determines the areas where financial resources should be directed. By leveraging AI technology in this manner, the project seeks to streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#goals-and-objectives",
    "href": "posts/Team_Blog_Week4/Team Blog.html#goals-and-objectives",
    "title": "Team Blog: Week Four",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nImage Gathering\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVangaurd: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\n\n\nBuild, Train, and Test AI Models\n\nVegetation Model\nSiding Model\nGutter Model\n\nCreate Database of Housing Information\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#our-progress",
    "href": "posts/Team_Blog_Week4/Team Blog.html#our-progress",
    "title": "Team Blog: Week Four",
    "section": "Our Progress",
    "text": "Our Progress\nWe have been making good progress to complete the goals and objectives we outlined above. Since the beginning of the Data Science for the Public Good Program, we have been expanding our knowledge of data science, particularly in areas that relate to this housing project. We have been learning and covering new concepts through Data Camp. We have also watched two webinars on TidyCensus training, as well as started creating AI Models to practice with.\n\nData Camp Training:\n\nGitHub Concepts\nAI Fundamentals\nIntroduction to R\nIntermediate R\nIntroduction to the Tidyverse\nWeb Scraping in R\nIntroduction to Deep Learning with Keras\n\n\n\nTidyCensus Demographic Data Collection:\nOne of the first steps in our project was to explore the available demographic data in our selected cities and counties. We thought it valuable to understand the demographic data, and we have represented in the plots below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Test AI Models:\nThe next step was creating an AI Model. We decided to create an AI Model early in the project before finishing the housing data collection so that we had a better understanding when it came to putting everything together. The AI Model below tests for vegetation in front of houses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Week:\n\nIn-Person Data Collection\nOn Tuesday this week, the entire DSPG program went to Slater to practice data collection in person. The housing group took this as an opportunity to collect some housing photos on the ground to use in our AI Model later on.\n\n\nGoogle Street View and URLs\nWe are getting the majority of our photos for the AI to use from Google Street View. Google has an API key that you can use to generate an image for a specific address. We spent the first half of this week pulling addresses from each of our cities and creating URLs to pull the images from Google Street View.\nWe ran into a couple of problems when doing this, the biggest of which is displayed in the images below. Because we are working with cities in rural areas, there is not Google Street View images available for every street in our cities.\n\n\n\nGoogle Street View information for Grundy Center, Iowa. For reference, population was 2,811 as of 2023.\n\n\n\n\n\nGoogle Street View information for Slater, Iowa. For reference, population was 1,639 as of 2023.\n\n\n\n\n\nGoogle Street View information for Independence, Iowa. For reference, population was 6,307 as of 2023.\n\n\n\n\n\nGoogle Street View information for New Hampton, Iowa. For reference, population was 3,368 as of 2023.\n\n\nBelow is a sample from the tables we created containing the URLs to grab the images from Google Street View.\n\n\n\n\nWeb Scraping\nOnce we were finished collecting addresses and generating URLs, we moved on to scraping the web for more images. We decided to grab images from Zillow, Realtors.com, and the County Assessor pages for our cities. We were able to successfully scrape images from Zillow this week.\n\n\n\n\n\n\n\n\n\n\nWhen web scraping, we ran into a problem with blurred houses. Upon some research, we found out that some home owners pay Google to have their home blurred for Google Street View.\n\n\n\n\n\nThe next websites we are scraping hold the Iowa Assessors housing data for our four cities. We found a webpage that has links to every counties assessor website. The key at the bottom shows where the data is held. Yellow means the data is held by Vanguard. Blue means the data is online. For most of Iowa’s counties, this means that it is held by Beacon Schneider.\n\n\n\n\n\n\n\n\nWeb Scraping Issues\nIndependence had issues where the house number was listed as 100/101 and also had 100 1/2. Thankfully the function to grab the Google images ran all the way but said there were 50 errors including both addresses with two house numbers and with half signs. If you remove one of the address numbers or remove the 1/2 (basically removing the / sign) the image url still brings you to a Google image. We could possibly go back through and grab these URL’s and alter them to try and grab these addresses if necessary.\n\nHappies\n\nhad a great meeting with Erin Olson-Douglas\nfinished collecting and creating URL addresses for Google Street View images\nZillow owns Trulia so we don’t have to web scrape both sites :)\nable to successfully scrape some things from Zillow !\n\n\n\nCrappies\n\nWeb Scraping\nBeacon and Vanguard have anti-web scraping protections\nAngelina’s Excel is stupid"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#future-plans-and-next-steps",
    "href": "posts/Team_Blog_Week4/Team Blog.html#future-plans-and-next-steps",
    "title": "Team Blog: Week Four",
    "section": "Future Plans and Next Steps",
    "text": "Future Plans and Next Steps\nOnce we are able to scrape enough images off of Zillow, Realtors.com, and the assessor pages, we will be able to move on with creating AI Models. The diagram below outlines how the AI Models will work in the next steps of out project."
  },
  {
    "objectID": "posts/Week 6/Week_6.html",
    "href": "posts/Week 6/Week_6.html",
    "title": "Week Six of Data Science for the Public Good",
    "section": "",
    "text": "On Monday and Tuesday of this week, we traveled to Grundy Center, New Hampton and Independence to do some WINVEST work. Our task was to conduct neighborhood scoring of the communities. I learned on Tuesday that the purpose of our scoring was to get the communities a grant for neighborhood funding. These three communities had all applied to a Housing and Urban Development (HUD) grant, and it was our job to gather evidence of need for each community. Based on the sections of the community that I evaluated, I think Independence should receive the grant.\nWe evaluated the following characteristics of the houses based on a good, fair, poor scale:\n\nhouse condition\nlot condition\nsidewalk\ngutters\nroof\nsiding\nlandscape\n\nWe also wanted to know if there was junk present on the lot, if the image of the house was obstructed, and what was causing the obstruction, if any.\nBelow are some images I took of particularly low quality houses that I thought our AI Models would benefit to train on.\n\n\n\nPoor roof, poor gutter, and fine siding.\n\n\n\n\n\nFine roof, poor siding, and fine landscape.\n\n\n\n\n\nThere are plants growing out of this gutter."
  },
  {
    "objectID": "posts/Week 6/Week_6.html#monday-and-tuesday",
    "href": "posts/Week 6/Week_6.html#monday-and-tuesday",
    "title": "Week Six of Data Science for the Public Good",
    "section": "",
    "text": "On Monday and Tuesday of this week, we traveled to Grundy Center, New Hampton and Independence to do some WINVEST work. Our task was to conduct neighborhood scoring of the communities. I learned on Tuesday that the purpose of our scoring was to get the communities a grant for neighborhood funding. These three communities had all applied to a Housing and Urban Development (HUD) grant, and it was our job to gather evidence of need for each community. Based on the sections of the community that I evaluated, I think Independence should receive the grant.\nWe evaluated the following characteristics of the houses based on a good, fair, poor scale:\n\nhouse condition\nlot condition\nsidewalk\ngutters\nroof\nsiding\nlandscape\n\nWe also wanted to know if there was junk present on the lot, if the image of the house was obstructed, and what was causing the obstruction, if any.\nBelow are some images I took of particularly low quality houses that I thought our AI Models would benefit to train on.\n\n\n\nPoor roof, poor gutter, and fine siding.\n\n\n\n\n\nFine roof, poor siding, and fine landscape.\n\n\n\n\n\nThere are plants growing out of this gutter."
  },
  {
    "objectID": "posts/Week 6/Week_6.html#wednesday",
    "href": "posts/Week 6/Week_6.html#wednesday",
    "title": "Week Six of Data Science for the Public Good",
    "section": "Wednesday",
    "text": "Wednesday\nOn Wednesday, I started the demographic profile of Grundy Center, New Hampton and Independence. Morenike, my project leader had requested them, so she could put them in our final report.\nI looked at the following characteristics of the communities:\n\nHome ownership rate\nPopulation\nMedian income\nAge of houses\nHouse value\n\nI used 5-Year ACS estimates for all of my plots. The ACS gives more detailed information on demographics of communities. I chose 5-Year over 1-Year estimates because 1-Year estimates are not available for communities under 65,000 in population. 5-Year estimates are also more accurate.\nI was able to finish the population plots on Wednesday. I first looked at the total population of each community.\n\n\n\n\n\nThen I was interested in the percent change in population each community had experienced since 1990. Unfortunately, 1990 Census data has been pulled from TidyCensus. There is a way to access it, but that is not something I had time to explore.\nI would like to go back and change this plot so that the 0.0% change for 2000 is not visible.\n\n\n\n\n\nFinally, I calculated a population projection for 2030 using the AAAC method. AAAC stands for average annual absolute change.\n\nAAAC = (population 1 - population 2) / time\nPopulation projection = population + (time * AAAC)"
  },
  {
    "objectID": "posts/Week 6/Week_6.html#thursday",
    "href": "posts/Week 6/Week_6.html#thursday",
    "title": "Week Six of Data Science for the Public Good",
    "section": "Thursday",
    "text": "Thursday\nOn Thursday, I finished the plots for income and housing value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI spent some time Thursday afternoon searching the US Census website for API codes.\nLink to 2021 5-Year ACS API codes: https://api.census.gov/data/2021/acs/acs5/variables.html\nYear Structure Built variables\n\n\n\nVariable Name\nAPI Code\n\n\n\n\nTotal\nB25034_001\n\n\n2020 or later\nB25034_002\n\n\n2010-2019\nB25034_003\n\n\n2000-2009\nB25034_004\n\n\n1990-1999\nB25034_005\n\n\n1980-1989\nB25034_006\n\n\n1970-1979\nB25034_007\n\n\n1960-1969\nB25034_008\n\n\n1950-1959\nB25034_009\n\n\n1940-1949\nB25034_010\n\n\n1939 or earlier\nB25034_011\n\n\nMedian structure age\nB25035_001\n\n\n\nNumber of Bedrooms = B25041_001 through B25041_007\nMedian House Value by Year Structure Built = B25107_001 through B25107_011\nTotal Housing Units = B25002_001\nOccupied Housing Units = B25002_002\nVacant Housing Units = B25002_003\nVacancy Status\n\n\n\nVariable Name\nAPI Code\n\n\n\n\nFor rent\nB25004_002\n\n\nRented, not occupied\nB25004_003\n\n\nFor sale\nB25004_004\n\n\nSold, not occupied\nB25004_005\n\n\nSeasonal\nB25004_006\n\n\nMigrant workers\nB25004_007\n\n\nOther\nB25004_008"
  },
  {
    "objectID": "posts/Week 6/Week_6.html#friday",
    "href": "posts/Week 6/Week_6.html#friday",
    "title": "Week Six of Data Science for the Public Good",
    "section": "Friday",
    "text": "Friday\nOn Friday, I finished the rest of the plots for the demographic profile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n** insert year built plots **\n\n\n\n\n\n\n\n\n\n\nAfter our Friday meeting, I got some notes from Chris and other members of DSPG on my plots. They suggested I change some of my plots to display percentages instead of counts. This change will give a more accurate representation of the data.\nI will be making these changes next week, and I will be starting the demographic analysis to determine which communities in Iowa could benefit from our project."
  },
  {
    "objectID": "posts/Week_1/Week_1.html",
    "href": "posts/Week_1/Week_1.html",
    "title": "Week One of Data Science for the Public Good",
    "section": "",
    "text": "The priority for week one of Data Science for the Public Good was DataCamp training. I completed the following list of DataCamp courses this week."
  },
  {
    "objectID": "posts/Week_1/Week_1.html#datacamp-trainings",
    "href": "posts/Week_1/Week_1.html#datacamp-trainings",
    "title": "Week One of Data Science for the Public Good",
    "section": "DataCamp Trainings",
    "text": "DataCamp Trainings\n\nAI Fundamentals\nGitHub Concepts\nR Programming Assessment\nUnderstanding and Interpreting Data Assessment\nIntroduction to R\nIntroduction to the Tidyverse"
  },
  {
    "objectID": "posts/Week_2/Week_2.html",
    "href": "posts/Week_2/Week_2.html",
    "title": "Week Two",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa &lt;- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %&gt;% \n  mutate(tenure = \"Owner\") %&gt;% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa &lt;- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %&gt;% \n  mutate(tenure = \"Renter\") %&gt;% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa &lt;- rent_iowa %&gt;% \n  bind_rows(own_iowa)%&gt;% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage &lt;- c(\"medage\" = \"B01002_001\")\n\ngrundy &lt;- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia &lt;- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 &lt;- grundy %&gt;% \n  bind_rows(ia,inde, new) %&gt;% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign &lt;- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %&gt;% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign &lt;- foreign %&gt;% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs &lt;- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house &lt;- grundy_acs %&gt;% \n  bind_rows(inde_acs,new_acs)\nmed_house %&gt;%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un &lt;- grundy_un %&gt;% \n  bind_rows(inde_un,new_un)\npct_un %&gt;%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders.\ngit_add\ngit_commit\ngit_push"
  },
  {
    "objectID": "posts/Week_2/Week_2.html#tidycensus-practice",
    "href": "posts/Week_2/Week_2.html#tidycensus-practice",
    "title": "Week Two",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa &lt;- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %&gt;% \n  mutate(tenure = \"Owner\") %&gt;% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa &lt;- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %&gt;% \n  mutate(tenure = \"Renter\") %&gt;% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa &lt;- rent_iowa %&gt;% \n  bind_rows(own_iowa)%&gt;% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage &lt;- c(\"medage\" = \"B01002_001\")\n\ngrundy &lt;- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia &lt;- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 &lt;- grundy %&gt;% \n  bind_rows(ia,inde, new) %&gt;% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign &lt;- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %&gt;% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign &lt;- foreign %&gt;% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs &lt;- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house &lt;- grundy_acs %&gt;% \n  bind_rows(inde_acs,new_acs)\nmed_house %&gt;%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un &lt;- grundy_un %&gt;% \n  bind_rows(inde_un,new_un)\npct_un %&gt;%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders.\ngit_add\ngit_commit\ngit_push"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html",
    "href": "posts/Week_3/Week_Three.html",
    "title": "Week Three of Data Science for the Public Good",
    "section": "",
    "text": "This week my team has been trying to gather resources to use in our AI model. We are trying to scrape data from multiple different sources including Beacon, Vanguard, and Trulia to compile photos and train our AI model.\nI have been learning how to web scrape, and I have completed the following DataCamp trainings:"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#datacamp-trainings",
    "href": "posts/Week_3/Week_Three.html#datacamp-trainings",
    "title": "Week Three of Data Science for the Public Good",
    "section": "DataCamp Trainings",
    "text": "DataCamp Trainings\n\nIntermediate R\nWeb Scraping in R\n\nI have also created an R Markdown file to document my web scraping practice.\nI did a quick Google search for a web scraper for Beacon before I attempted it myself, and I found a GitHub page dedicated to one:\nhttps://github.com/openaddresses/machine/issues/580\nI am not sure it is relevant to the data I am trying to scrape from Beacon.\nI also learned how to create a quarto blog this week!"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#ai-modeling",
    "href": "posts/Week_3/Week_Three.html#ai-modeling",
    "title": "Week Three of Data Science for the Public Good",
    "section": "AI Modeling",
    "text": "AI Modeling\nTo better understand the AI model my group is trying to create, I am watching the following YouTube videos:\n\nhttps://www.youtube.com/watch?v=19LQRx78QVU&list=PLgNJO2hghbmiXg5d4X8DURJP9yv9pgjIu&index=1&ab_channel=NicholasRenotte\nhttps://www.youtube.com/watch?v=jztwpsIzEGc&ab_channel=NicholasRenotte"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#morning-coffee-talk",
    "href": "posts/Week_3/Week_Three.html#morning-coffee-talk",
    "title": "Week Three of Data Science for the Public Good",
    "section": "Morning Coffee Talk",
    "text": "Morning Coffee Talk\nI also gave the Morning Coffee Talk on Thursday this week over the Des Moines Housing Project I was a part of. The Des Moines Housing Project was conducted by czb, a firm located in Bath, Maine. I was hired as a student researcher for them this past spring, and I conducted housing surveys on roughly 6,000 properties in Southwestern Des Moines."
  },
  {
    "objectID": "posts/Week_5/Week_5.html",
    "href": "posts/Week_5/Week_5.html",
    "title": "Week Five of Data Science for the Public Good",
    "section": "",
    "text": "Chris informed us this morning that we need to include a demographic analysis or report in our project. We reviewed the initial project brief, and I realized that my group had skipped to the end of Year 1 when we started with AI Models. Whoops!\nWe need to go back at some point and work with demographics. I started a little bit of it today. We need to include the following:\n\nIdentify communities with populations between 500 and 10,000\nChange in population\nPresence of schools\nMean age of residents (I wonder if we should discuss changing this to median because it is less influenced by outliers)\nIndustry report\nAg Census data:\n\nNumber of Operators\nOperator Owned\netc.\n\nHousing appreciation and depreciation vs. inflation\n\nWe also started sorting the Google Street View images to train our AI Models on Monday. We need to sort based on six different models. We started with our first three.\n\nIs a house present?\n\nYes\nNo\n\nIs it a clear image of a house?\n\nObstructed\nPartially obstructed\nNot obstructed\n\nAre there multiple houses?\n\nOne House\nMore than one house\n\n\nWhen we started sorting the images, I noticed an error with an Independence image. The image on the left below is the one that we downloaded from Google Street View. Because I have been in Independence before, I could tell that this was not a photo of Independence. The photo on the left is from a different place, and the photo on the right is the same address but actually in Independence.\nI checked the URL we used to generate the Google Street View image on the left, and I noticed that we did not specify the city and state. In fact, we didn’t specify the city and state for any of the Independence URLs or the New Hampton URLs. Thankfully this was a quick fix. We just added the city and state to the URL files, and the images downloaded quickly.\n\n\n\nError in Independence Address for Google Image API\n\n\nWhile I was waiting for images to download on Monday, I started working on scraping Realtor.com. NOTE: it is Realtor.com not Realtors.com. I definitely have been misspelling it. From the brief look I took at web scraping Realtor.com, it looks like it might be slightly more complicated than Zillow.\nThe address data is stored differently on Realtor.com, and I was not successful in scraping it. Instead of being on one line of HTML, it is split up on multiple.\n\n\n\nRealtor.com address HTML"
  },
  {
    "objectID": "posts/Week_5/Week_5.html#monday",
    "href": "posts/Week_5/Week_5.html#monday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "",
    "text": "Chris informed us this morning that we need to include a demographic analysis or report in our project. We reviewed the initial project brief, and I realized that my group had skipped to the end of Year 1 when we started with AI Models. Whoops!\nWe need to go back at some point and work with demographics. I started a little bit of it today. We need to include the following:\n\nIdentify communities with populations between 500 and 10,000\nChange in population\nPresence of schools\nMean age of residents (I wonder if we should discuss changing this to median because it is less influenced by outliers)\nIndustry report\nAg Census data:\n\nNumber of Operators\nOperator Owned\netc.\n\nHousing appreciation and depreciation vs. inflation\n\nWe also started sorting the Google Street View images to train our AI Models on Monday. We need to sort based on six different models. We started with our first three.\n\nIs a house present?\n\nYes\nNo\n\nIs it a clear image of a house?\n\nObstructed\nPartially obstructed\nNot obstructed\n\nAre there multiple houses?\n\nOne House\nMore than one house\n\n\nWhen we started sorting the images, I noticed an error with an Independence image. The image on the left below is the one that we downloaded from Google Street View. Because I have been in Independence before, I could tell that this was not a photo of Independence. The photo on the left is from a different place, and the photo on the right is the same address but actually in Independence.\nI checked the URL we used to generate the Google Street View image on the left, and I noticed that we did not specify the city and state. In fact, we didn’t specify the city and state for any of the Independence URLs or the New Hampton URLs. Thankfully this was a quick fix. We just added the city and state to the URL files, and the images downloaded quickly.\n\n\n\nError in Independence Address for Google Image API\n\n\nWhile I was waiting for images to download on Monday, I started working on scraping Realtor.com. NOTE: it is Realtor.com not Realtors.com. I definitely have been misspelling it. From the brief look I took at web scraping Realtor.com, it looks like it might be slightly more complicated than Zillow.\nThe address data is stored differently on Realtor.com, and I was not successful in scraping it. Instead of being on one line of HTML, it is split up on multiple.\n\n\n\nRealtor.com address HTML"
  },
  {
    "objectID": "posts/Week_5/Week_5.html#tuesday",
    "href": "posts/Week_5/Week_5.html#tuesday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Tuesday",
    "text": "Tuesday\nOn Tuesday, we continued sorting images to train our AI Models. I finished sorting the Slater images for a clear view of the house yesterday, and today I worked on the same sorting for Grundy Center.\nWe need about 200 images for each category to train our model. We are struggling to find enough “bad” images.\nChris came over to talk to us about the app he is creating for the Housing Team. His app is meant to make image sorting easier, so he wants to be able to get it to us as soon as possible. He needed a database created with the house address, Google Street View image URL, and city to finish the app. I created the housing database for all the addresses for Slater, Independence, Grundy Center, and New Hampton.\n\n\n\n\n\nChris also suggested that Angelina and I take a look at spatial graphing in R. He said to try the DataCamp course Geospatial in R, and he said to look up Kyle Walkers TidyCensus book.\nOn Tuesday, I also worked more on the demographics analysis that Chris asked for. I am starting the analysis by looking at total population in Iowa’s communities. The first plot I created plotted the change in population for Iowa as a whole.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidycensus)\nlibrary(ggthemes)\n\n# renaming the variables now so I don't have to do it later\npop00 &lt;- c(\"pop\" = \"P001001\")\npop10 &lt;- c(\"pop\" = \"P001001\")\npop20 &lt;- c(\"pop\" = \"P1_001N\")\n\n# iowa 2000 population\niowa00 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        year = 2000,\n                        output = \"wide\",\n                        variable = pop00) %&gt;% \n  mutate(year = 2000)\n\nGetting data from the 2000 decennial Census\nUsing Census Summary File 1\n\n# iowa 2010 population\niowa10 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        variable = pop10,\n                        year = 2010,\n                        output = \"wide\") %&gt;% \n  mutate(year = 2010)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n# iowa 2020 population\niowa20 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        variable = pop20,\n                        year = 2020,\n                        output = \"wide\") %&gt;% \n  mutate(year = 2020)\n\nGetting data from the 2020 decennial Census\nUsing the PL 94-171 Redistricting Data summary file\nNote: 2020 decennial Census data use differential privacy, a technique that\nintroduces errors into data to preserve respondent confidentiality.\ni Small counts should be interpreted with caution.\ni See https://www.census.gov/library/fact-sheets/2021/protecting-the-confidentiality-of-the-2020-census-redistricting-data.html for additional guidance.\n\n# bind 2000-2020 data together\niowa &lt;- iowa20 %&gt;% \n  bind_rows(iowa10,iowa00)\n\n# plot it\nchange_pop_iowa.jpg &lt;- iowa %&gt;% \n  ggplot(aes(x = year, y = pop))+\n  geom_line()+\n  geom_point(size = 2)+\n  geom_text(aes(label = scales::comma(pop)), hjust = -.25)+  \n  scale_y_continuous(label = scales::comma)+  # how do I change the size of the axis labels?\n  scale_x_continuous(limits = c(1998, 2025),\n                     breaks = c(2000,2010,2020))+\n  theme_fivethirtyeight() +  \n  theme(legend.position = \"bottom\")+\n  labs(title = \"Change in Total Population\",\n       subtitle = \"State of Iowa\",\n       y = \"Population\",\n       x = \"\",\n       color = \"\", \n       caption = \"2000-2020 Decennial Census\")\n\n# save the plot as a .jpg\nchange_pop_iowa.jpg %&gt;% ggsave(filename = \"change_pop_iowa.jpg\",width = 6,height = 6, dpi = 400)\n\n\n\n\n\n\nNext, I pulled the total population for each individual community in Iowa for 2020, 2010, and 2000. Using the 2020 Decennial Census data, I found that there are 408 communities in Iowa that fall within our population parameters. This is 39.69% of all cities in Iowa.\nI also looked at the growth rate for communities in Iowa. Because most of Iowa’s cities are on the smaller size, I don’t think that population is an effective indicator for this project. In fact, the median population for communities in Iowa is 384. I added a column for growth rate to my cities data frame containing the communities in Iowa that fall between our population parameters. Growth rate is calculated by dividing the change in population by the time period the change occurred.\n\nGrowth Rate = N/t\n\nI also plotted the twenty-five lowest growth rates in Iowa.\n\n\n\n\n\nKeokuk really stands out on this plot. Earlier in the Housing project, there was discussion about the deteriorating qualities of Lee County, the county of which Keokuk resides. This anomaly will need further investigated.\nNext, I want to look at which communities are growing, stable, and shrinking. The growth rates column should aid in this analysis."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#wednesday",
    "href": "posts/Week_5/Week_5.html#wednesday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Wednesday",
    "text": "Wednesday\nBecause I will be doing a lot of data visualization in the coming weeks with my demographics analysis task, I decided it would be a good idea to refresh my data visualization skills. I scored a 116 on the Data Visualization in R assessment I took on DataCamp. I am pretty happy with that score.\nI feel that I have quality skills in terms of data visualization, but I am lacking when it comes to organizing the data for visualizations. I think completely the track for Data Visualization in R would be helpful.\n\n\n\n\n\nWhile I was on DataCamp on Wednesday, I also completed the Introduction to Deep Learning with Keras course so I would be ready to create an AI Model. I am in charge of making the model that identifies clear images of houses. We are using Google Collab to create our models, and all the data is being stored in Google Drive. The “clear images” are sorted into three categories: Obstructed, Partially Obstructed, and Not Obstructed.\n\n\n\n\n\nI was successful in creating the AI Model on Wednesday. Below is the accuracy of my model. There is still a long way to go with it. The Housing team has a lot more photos to gather to train our models on and make them as accurate as possible. Now that our AI Models are created, we can go back to web scraping and gathering images."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#thursday",
    "href": "posts/Week_5/Week_5.html#thursday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Thursday",
    "text": "Thursday\nITAG Conference\nWork on Data Visualization in R DataCamp track."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#friday",
    "href": "posts/Week_5/Week_5.html#friday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Friday\\",
    "text": "Friday\\"
  },
  {
    "objectID": "posts/Week_7/Week_7.html",
    "href": "posts/Week_7/Week_7.html",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "",
    "text": "On Monday, my group had our final? meeting with our stakeholders, Erin Mullinex and Erin Olson-Douglas. I got some clarification on what they expected from the demographic analysis from the meeting. They wanted me to look further into economic and fiscal conditions of Iowa’s communities.\nErin and Erin’s intention with our project is to improve the housing stock across the state. To do so, we need to identify which communities need improvement based on external factors. The final destination after three years is to create an application that assess housing conditions and generates a map of housing characteristic quality.\nErin and Erin also let me know that thriving communities tend to have an employer of some sort. Communities with an employer do well because the employer may provide grants and funding to improve the community. Thriving communities are also growing in population and sometimes have lending institutions and non-profits.\nSo far, I know to look at the following characteristics in the communities that could benefit from our project:\n\npopulations between 500 and 10,000\npopulation change from 2000-2020: growing, stable, or shrinking?\nmedian age of residents\npresence of ag-related industries\nhome ownership rates\npercent of residents commuting to work\nnumber of jobs\nmedian income\nincome change\nworkforce characteristics (this one is still confusing)\nlocal spending patterns (don’t know where to find this data yet)\n\nAfter the meeting with Erin and Erin, I spent the rest of the day sorting WINVEST photos to train our AI Models. The photos are stored in CyBox under the communities they are associated with. Chris made a CSV file of the WINVEST data that we used to filter for poor roofs, poor siding, poor landscaping, and poor gutter. The CSV has the image name in a column, so we could then manually search for the image names associated with the poor values. I also went to Durham’s GIS lab to create community summary infographics for Independence, New Hampton, and Grundy Center.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity summary infographics are really easy to make.\n\nDownload shapefile for the geography you want to make an infographic for. For me, this was the places geography that I downloaded from the Census’s shapefile directory.\nInsert the geography shapefile into ArcGIS Pro.\nSearch for infographics in the tool bar.\nClick on the geography you want to make an infographic for.\nSelect which type of infographic you would like. There are a lot of pre-loaded infographics available for free on ArcGIS Pro. I chose the Community Summary template to create the infographics above.\n\nSome of the elements on the infographics you can toggle to compare to different geographies. The Iowa community summary had the option to compare to the United States. The city infographics I could choose to compare to the county, state, or nation."
  },
  {
    "objectID": "posts/Week_7/Week_7.html#tidycensus-practice",
    "href": "posts/Week_7/Week_7.html#tidycensus-practice",
    "title": "Week Two of Data Science for the Public Good",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa &lt;- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %&gt;% \n  mutate(tenure = \"Owner\") %&gt;% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa &lt;- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %&gt;% \n  mutate(tenure = \"Renter\") %&gt;% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa &lt;- rent_iowa %&gt;% \n  bind_rows(own_iowa)%&gt;% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage &lt;- c(\"medage\" = \"B01002_001\")\n\ngrundy &lt;- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia &lt;- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 &lt;- grundy %&gt;% \n  bind_rows(ia,inde, new) %&gt;% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign &lt;- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %&gt;% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign &lt;- foreign %&gt;% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs &lt;- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house &lt;- grundy_acs %&gt;% \n  bind_rows(inde_acs,new_acs)\nmed_house %&gt;%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un &lt;- grundy_un %&gt;% \n  bind_rows(inde_un,new_un)\npct_un %&gt;%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nI wish I would’ve gotten to dig deeper into TidyCensus this week, but I was on vacation in Vegas. I learned a lot about TidyCensus considering though.\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders."
  },
  {
    "objectID": "posts/Week_Four/Week_4.html",
    "href": "posts/Week_Four/Week_4.html",
    "title": "Week Four of Data Science for the Public Good",
    "section": "",
    "text": "On Monday this week, we met with the Director of the Community and Economic Development department at Iowa State University Extension and Outreach, Erin Olson-Douglas. She is one of our stakeholders for the Housing Project, and we had a great meeting with her. The agenda of the meeting was to update her on where we are at with the project. As of Monday, we were gathering data and ideas for the AI Image Model.\nShe had a couple of questions:\n\nIf the image gathered from Google Street View is bad, can the model be told to look at a different site for a better image?\nHow will the model choose one of the photos?\n\nIs the image chosen on a rating scale (good, usable, not usable)?\nIs the image chosen based on a simple yes or no?\n\n\nI ended the meeting with a question myself:\n\nHow do we get the model to work off of websites, not already available, pre-loaded images?\n\nThis is something I had not thought about yet. So far, we have been making, training, and testing models on images that we gathered and put into a database ourselves. I am not sure how we would get an AI Model to gather its own images, or if that is something we even need it to do.\nWe also ended the meeting with some knowledge on how we are presenting our final project. We need to keep in mind how our project can be picked up and used by others. We also need to make sure we are documenting and explaining our process so the next years of DSPG interns can continue this housing project.\nFinally, Erin Olson-Douglas is going to be arranging a meeting with a county assesor for us. We are very curious how assessors complete their jobs with houses. We want to know what it is that they look at and look for when completing the assessment. Erin thinks we will be meeting with the Polk County assessor.\n\n\n\nOn Monday, I also helped my team assemble Excel spreadsheets with all the addresses for Slater, Grundy Center, Independence, and New Hampton. I was in charge of the Slater data and part of the Grundy Center and Independence data sets.\nWe needed to first gather the addresses for each city. Gavin and Angelina used a spatial tool on Vanguard and Beacon to do this. On the map for the websites, the second tool from the left is called the Selection Tool. When you drag it over a section of properties, the list of parcels shows up in a “Results” section on the right.\nGavin used a web scraper attached to a Chrome Extension to then scrape the parcel information listed in “Results” from Beacon. Angelina was lucky and could download the parcel information as a .csv file from Vanguard.\n\n\n\n\n\nOnce the parcel information was scraped for Beacon, we had to go in and clean the data. Below is a sample of what the .csv file looked like after Gavin scraped it. The first thing we had to do was get the data all on one line. We used the following function in Excel to transform the street addresses onto one line:\n\n=TRIM(CLEAN(SUBSTITUTE(cell, CHAR(160), ” “)))\n\n\n\n\n\n\nWe then used the the “Text to Columns” tool in Excel to separate the data into Parcel ID, Street Address, and Owner. We used the “-” as a delimiter. We cleaned the data to remove any addresses that were obviously non-residential, and we also narrowed the data down to just the Parcel ID and address.\n\n\n\n\n\nFrom there, we created the URLs to gather Google Street View images.\nWe used the following function in Excel to transform the street addresses into workable addresses for Google Street View:\n\n=SUBSTITUTE(TRIM(cell),” “,”+“))\n\nThis function removes all spaces and replaces them with + signs.\nHere is the output of the cleaned Slater data:"
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#monday",
    "href": "posts/Week_Four/Week_4.html#monday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "",
    "text": "On Monday this week, we met with the Director of the Community and Economic Development department at Iowa State University Extension and Outreach, Erin Olson-Douglas. She is one of our stakeholders for the Housing Project, and we had a great meeting with her. The agenda of the meeting was to update her on where we are at with the project. As of Monday, we were gathering data and ideas for the AI Image Model.\nShe had a couple of questions:\n\nIf the image gathered from Google Street View is bad, can the model be told to look at a different site for a better image?\nHow will the model choose one of the photos?\n\nIs the image chosen on a rating scale (good, usable, not usable)?\nIs the image chosen based on a simple yes or no?\n\n\nI ended the meeting with a question myself:\n\nHow do we get the model to work off of websites, not already available, pre-loaded images?\n\nThis is something I had not thought about yet. So far, we have been making, training, and testing models on images that we gathered and put into a database ourselves. I am not sure how we would get an AI Model to gather its own images, or if that is something we even need it to do.\nWe also ended the meeting with some knowledge on how we are presenting our final project. We need to keep in mind how our project can be picked up and used by others. We also need to make sure we are documenting and explaining our process so the next years of DSPG interns can continue this housing project.\nFinally, Erin Olson-Douglas is going to be arranging a meeting with a county assesor for us. We are very curious how assessors complete their jobs with houses. We want to know what it is that they look at and look for when completing the assessment. Erin thinks we will be meeting with the Polk County assessor.\n\n\n\nOn Monday, I also helped my team assemble Excel spreadsheets with all the addresses for Slater, Grundy Center, Independence, and New Hampton. I was in charge of the Slater data and part of the Grundy Center and Independence data sets.\nWe needed to first gather the addresses for each city. Gavin and Angelina used a spatial tool on Vanguard and Beacon to do this. On the map for the websites, the second tool from the left is called the Selection Tool. When you drag it over a section of properties, the list of parcels shows up in a “Results” section on the right.\nGavin used a web scraper attached to a Chrome Extension to then scrape the parcel information listed in “Results” from Beacon. Angelina was lucky and could download the parcel information as a .csv file from Vanguard.\n\n\n\n\n\nOnce the parcel information was scraped for Beacon, we had to go in and clean the data. Below is a sample of what the .csv file looked like after Gavin scraped it. The first thing we had to do was get the data all on one line. We used the following function in Excel to transform the street addresses onto one line:\n\n=TRIM(CLEAN(SUBSTITUTE(cell, CHAR(160), ” “)))\n\n\n\n\n\n\nWe then used the the “Text to Columns” tool in Excel to separate the data into Parcel ID, Street Address, and Owner. We used the “-” as a delimiter. We cleaned the data to remove any addresses that were obviously non-residential, and we also narrowed the data down to just the Parcel ID and address.\n\n\n\n\n\nFrom there, we created the URLs to gather Google Street View images.\nWe used the following function in Excel to transform the street addresses into workable addresses for Google Street View:\n\n=SUBSTITUTE(TRIM(cell),” “,”+“))\n\nThis function removes all spaces and replaces them with + signs.\nHere is the output of the cleaned Slater data:"
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#tuesday",
    "href": "posts/Week_Four/Week_4.html#tuesday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Tuesday",
    "text": "Tuesday\nOn Tuesday morning, the entire DSPG group went to Slater to get some practice for the WINVEST project. We walked around with a city council member so we would have some local knowledge of the town while we were practicing. The local knowledge was very helpful, as we would not have known what some of the downtown commercial buildings were used for without the city council member.\nWe discovered on Tuesday that gutters are not the best test of our AI Model because most houses in Slater either had perfectly fine gutters or had no gutters at all. We do not think we will be able to get enough images to successfully train an AI Model to identify damaged gutters.\nTuesday afternoon was spent web scraping. I did the DataCamp training for web scraping in R about a week and a half ago, which was understandable then. It could have been better when applying it in real-world scenarios. Angelina and I have done a lot of Googleing to find other examples to help us. We were tasked earlier with scraping county housing assessor data. Independence in Buchanan County is on Vanguard. Slater in Story County, Grundy Center in Grundy County, and New Hampton in Chickasaw County are all on Beacon. We needed help figuring out how to scrape from these sites this week.\nI successfully scraped the categories of shoes from my favorite shoe site, Jonak, though.\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.1.3\n\njonak = \"https://www.jonak-paris.com/collection/shoes/sandals.html\"\ncategories &lt;- read_html(jonak) %&gt;% html_elements(\".categ_itm_name\") %&gt;% html_text2()\nhead(categories)\n\n[1] \"New in\"              \"Mules, Clogs\"        \"Sandals\"            \n[4] \"Beach sandals\"       \"Wedges, Espadrilles\" \"Babies, salomes\"    \n\n\nI wanted to know if Beacon and Vanguard had anti-web scraping protections on them, and that’s why Angelina and I were unsuccessful in scraping them. I found a function online called paths_allowed() in the robotstxt package that checks to see if there are protections. Both Beacon and Vanguard have protections from running the URLs through the function. Jonak doesn’t, so it was easy to scrape from the site. Zillow doesn’t have any protections, either.\n\nlibrary(robotstxt)\n\nWarning: package 'robotstxt' was built under R version 4.1.3\n\n#TRUE = web scraping allowed, FALSE = web scraping not allowed\npaths_allowed(\"https://beacon.schneidercorp.com/Application.aspx?AppID=165&LayerID=2145&PageTypeID=3&PageID=1107&Q=1818183221\")\n\n\n beacon.schneidercorp.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://buchanan.iowaassessors.com/results.php?mode=basic&history=-1&ipin=%25&idba=&ideed=&icont=&ihnum=&iaddr=&ilegal=&iacre1=&iacre2=&iphoto=0\")\n\n\n buchanan.iowaassessors.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://www.zillow.com/homedetails/2925-Arbor-St-Ames-IA-50014/93961907_zpid/\")\n\n\n www.zillow.com                      \n\n\n[1] TRUE\n\npaths_allowed(\"https://www.jonak-paris.com/collection/shoes/sandals.html\")\n\n\n www.jonak-paris.com                      \n\n\n[1] TRUE\n\n\nBecause Zillow doesn’t have protections, the housing team decided to switch tactics. The Housing Team had decided earlier to scrape Zillow and Trulia alongside Vanguard and Beacon for housing data. When we started the scraping of Trulia, we learned that Zillow owns Trulia. This was a huge win for us because that meant we only had to scrape one of the sites. We chose Zillow because it provides Zestimates, estimates of housing price based on external factors, and Trulia does not."
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#wednesday-and-thursday",
    "href": "posts/Week_Four/Week_4.html#wednesday-and-thursday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Wednesday and Thursday",
    "text": "Wednesday and Thursday\nOn Wednesday, I was tasked with scraping the houses in Slater, IA. I made this data frame in R with the data I scraped from Zillow.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter()         masks stats::filter()\nx readr::guess_encoding() masks rvest::guess_encoding()\nx dplyr::lag()            masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#### Pulling recently SOLD houses ######\n########################################\n\nsold = \"https://www.zillow.com/slater-ia/sold/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%2C%22rs%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the url\nss = read_html(sold)\n\n# lists how many records there are to pull from\nhousesold &lt;- read_html(sold) %&gt;% html_elements(\"article\")\n\n#create a dataframe with addresses, prices, bathrooms, bedrooms, and square footage of all SOLD houses\nres_ss &lt;- tibble(\n      address= ss %&gt;% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;% html_text(),\n      price = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %&gt;% html_text(),\n      bedrooms = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %&gt;% \n        html_text(),\n      bathrooms = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %&gt;% \n        html_text(),\n      sqft = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %&gt;% \n        html_text()\n    ) \n\n    \n##### Pulling FOR SALE houses #####\n######################################\n\nsale = \"https://www.zillow.com/slater-ia/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the webpage\npg = read_html(sale)\n\n# get list of houses for sale that appears on the page\n# each property card is called an article when you inspect the webpage\nhousesale &lt;- read_html(sale)%&gt;%html_elements(\"article\")\n\n# create a dataframe for the FOR SALE houses\nres_pg &lt;- tibble(\n  address= pg %&gt;% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div[1]/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;% html_text(),\n  price = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %&gt;% html_text(),\n  bedrooms = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %&gt;% \n    html_text(),\n  bathrooms = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %&gt;% \n    html_text(),\n  sqft = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %&gt;% \n    html_text()\n) \n\n# combine recently SOLD and FOR SALE houses in one data frame\nresults &lt;- res_pg %&gt;% bind_rows(res_ss)\nprint(results)\n\n# A tibble: 12 x 5\n   address                             price    bedrooms bathrooms sqft \n   &lt;chr&gt;                               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;\n 1 201 10th Ave, Slater, IA 50244      $295,000 3        2         924  \n 2 50287 210th Hwy, Slater, IA 50244   $475,000 4        2         2,147\n 3 1013 Redbud Dr, Slater, IA 50244    $429,900 4        3         1,884\n 4 506 8th Ave, Slater, IA 50244       $328,000 4        3         1,180\n 5 604 Story St, Slater, IA 50244      $200,000 3        1         1,359\n 6 611 1st Ave N, Slater, IA 50244     $265,000 4        2         1,116\n 7 101 Main St, Slater, IA 50244       $255,000 5        3         1,896\n 8 1015 Redbud Dr, Slater, IA 50244    $397,732 2        3         1,325\n 9 107 Main St, Slater, IA 50244       $242,500 4        2         1,515\n10 52898 Highway 210, Slater, IA 50244 $400,000 4        1.75      2,550\n11 604 Marshall St, Slater, IA 50244   $135,000 3        1         1,166\n12 104 N Benton St, Slater, IA 50244   $210,000 3        2         1,056\n\n\nThis web scraping was really hard. I spent a lot of time understanding the xpaths and why I was using them. I think this was beneficial to me though. I got the xpaths for my code by inspecting the Zillow webpage. To inspect, you right click on the web page and then select “Inspect.” This will open up a screen that shows you the HTML in the web page.\nIf you right click on any element in the HTML you can select “Copy” and then “Full xpath” to copy the xpath of an element. There were some minor changes that needed to be made to the xpaths. Below is an example:\n\n/html/body/div[1]/div[5]/div/div/div/div/div[1]/ul/li[1]/div/div/article/div/div[1]/a/address\nv.\n/html/body/div[1]/div[5]/div/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/a/address\n\nThe difference in these two xpaths is what comes after the li element. The first xpath selects only the first li in the HTML. The second xpath selects all li elements in the HTML. The second version allows you to get all of the children of all li elements as well."
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#friday",
    "href": "posts/Week_Four/Week_4.html#friday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Friday",
    "text": "Friday\nI spent a lot of time on Thursday creating the Team blog for this week. We gave a presentation to an outside individual from Oklahoma State University Friday morning, so I made sure to put a lot of my attention towards that.\nAfter the presentation on Friday morning, I got back to web scraping. Gavin was able to scrape some images from Zillow on Thursday night, and he shared the code he used with Angelina and me. The code is below:\n\n# webpage to scrape. This specific link brings you to the grundy center houses for sale.\nzillow_url_grundy &lt;- \"https://www.zillow.com/grundy-center-ia/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22usersSearchTerm%22%3A%22Grundy%20Center%2C%20IA%22%2C%22mapBounds%22%3A%7B%22west%22%3A-93.21166512207031%2C%22east%22%3A-92.40828987792969%2C%22south%22%3A42.153050722920995%2C%22north%22%3A42.55594363773797%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A24980%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Afalse%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%7D\"\nwebpage_grundy &lt;- read_html(zillow_url_grundy)\n\n# gathers addresses. This xpath can be obtained by right clicking on the address you want and clicking inspect.\n# you then must navigate to the html section that contains the text. right click again and go to copy -&gt; full xpath\n# to gather all addresses on page the full xpath must be altered for example this xpath below has li// which signifies select all children where the original xpath would just have li/...\naddresses &lt;- webpage_grundy %&gt;%\n  html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div[1]/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;%\n  html_text()\nprint(addresses)\n\n\n# gathers image links. Similair method as above\nimage_urls &lt;- webpage_grundy %&gt;%\n  html_nodes(xpath = '//*[@id=\"swipeable\"]/div[1]/a/div/img') %&gt;%\n  html_attr(\"src\")\n\nprint(image_urls)\n\n#downloads first item\n#download.file(image_urls[1], \"image.png\", mode = \"wb\")\n\n# creates folder for images scraped then iterativly names each image (1-9 in this case).\n# More specifically it takes the image links gathered above, goes to each link, and downloads the image\n# dir.create makes a new folder. You only need to run this once.Everytime you do file.path to that folder it will add newly downloaded images to that folder\n# This method simply names each image 1 - number of images\ndir.create(\"images_grundy_sale\")\nfor (i in seq_along(image_urls)) {\n  file_path &lt;- file.path(\"images_grundy_sale\", paste0(\"image_\", i, \".png\"))\n  download.file(image_urls[i], file_path, mode = \"wb\")\n  print(file_path)\n}\n\n#creates folder for images scraped then names them based on address they were scraped with\n# same as above except for how the images are named. for each image the address grabbed earlier is printed as the name.\n# this returns (image_ 123 main st) for example\n# you can alter this to return our naming convention (source_city_address_) by replacing \"image_\" with the source and city\n# for example if you are pulling slater images from zillow it will be paste0(\"Z_S_\", address, \"_.png\") which will print the titles of images as Z_S_123 main st_.png\n# Z (Zillow) G (Google) V (Vanguard) B (Beacon) :: S (Slater) H (New Hampton) D (Independence) G (Grundy Center)\ndir.create(\"images_grundy_sale_addresses\")\nfor (j in seq_along(image_urls)) {\n  address &lt;- addresses[j]\n  file_name &lt;- paste0(\"image_\", address, \".png\")\n  file_path &lt;- file.path(\"images_grundy_sale_addresses\", file_name)\n  download.file(image_urls[j], file_path, mode = \"wb\")\n  print(file_path)\n}\n\nNext week, I will get around the anti-web scraping protections Beacon and Vanguard have on their sites. Beacon and Vanguard have information on houses that aren’t listed on Zillow, and more pictures aren’t listed on Zillow or Google Street View.\n\nNotes to self:\ngit pull\ngit add .\ngit commit -m “message”\ngit push"
  },
  {
    "objectID": "posts/Week_7/Week_7.html#monday",
    "href": "posts/Week_7/Week_7.html#monday",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "",
    "text": "On Monday, my group had our final? meeting with our stakeholders, Erin Mullinex and Erin Olson-Douglas. I got some clarification on what they expected from the demographic analysis from the meeting. They wanted me to look further into economic and fiscal conditions of Iowa’s communities.\nErin and Erin’s intention with our project is to improve the housing stock across the state. To do so, we need to identify which communities need improvement based on external factors. The final destination after three years is to create an application that assess housing conditions and generates a map of housing characteristic quality.\nErin and Erin also let me know that thriving communities tend to have an employer of some sort. Communities with an employer do well because the employer may provide grants and funding to improve the community. Thriving communities are also growing in population and sometimes have lending institutions and non-profits.\nSo far, I know to look at the following characteristics in the communities that could benefit from our project:\n\npopulations between 500 and 10,000\npopulation change from 2000-2020: growing, stable, or shrinking?\nmedian age of residents\npresence of ag-related industries\nhome ownership rates\npercent of residents commuting to work\nnumber of jobs\nmedian income\nincome change\nworkforce characteristics (this one is still confusing)\nlocal spending patterns (don’t know where to find this data yet)\n\nAfter the meeting with Erin and Erin, I spent the rest of the day sorting WINVEST photos to train our AI Models. The photos are stored in CyBox under the communities they are associated with. Chris made a CSV file of the WINVEST data that we used to filter for poor roofs, poor siding, poor landscaping, and poor gutter. The CSV has the image name in a column, so we could then manually search for the image names associated with the poor values. I also went to Durham’s GIS lab to create community summary infographics for Independence, New Hampton, and Grundy Center.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity summary infographics are really easy to make.\n\nDownload shapefile for the geography you want to make an infographic for. For me, this was the places geography that I downloaded from the Census’s shapefile directory.\nInsert the geography shapefile into ArcGIS Pro.\nSearch for infographics in the tool bar.\nClick on the geography you want to make an infographic for.\nSelect which type of infographic you would like. There are a lot of pre-loaded infographics available for free on ArcGIS Pro. I chose the Community Summary template to create the infographics above.\n\nSome of the elements on the infographics you can toggle to compare to different geographies. The Iowa community summary had the option to compare to the United States. The city infographics I could choose to compare to the county, state, or nation."
  },
  {
    "objectID": "posts/Week_7/Week_7.html#tuesday",
    "href": "posts/Week_7/Week_7.html#tuesday",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "Tuesday",
    "text": "Tuesday\nOn Tuesday, I was able to start the demographic analysis. I worked with population data and housing data from the 2021 5-Year American Community Survey.\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\n\n## Population Data ##\n# Getting population data from 2000-2020 Decennial Census.\n\n# The 2020 total population API is \"P1_001N.\"\n# The 2010 and 2000 total population API is \"P001001.\"\npop &lt;- get_decennial(geography = \"place\",\n                     state = \"IA\",\n                     variables = c(\"pop20\" = \"P1_001N\"),\n                     year = 2020,\n                     output = \"wide\")\npop10 &lt;- get_decennial(geography = \"place\",\n                state = \"IA\",\n                variable = c(\"pop10\"=\"P001001\"),\n                year = 2010,\n                output = \"wide\")\npop00 &lt;- get_decennial(geography = \"place\",\n                       state = \"IA\",\n                       variable = c(\"pop00\"=\"P001001\"),\n                       year = 2000,\n                       output = \"wide\")\n# Join the 2020, 2010, and 2000 total population data by GEOID and NAME.\npop &lt;- pop %&gt;% \n  left_join(pop10,by = c(\"GEOID\",\"NAME\"))\npop &lt;- pop %&gt;% \n  left_join(pop00,by = c(\"GEOID\",\"NAME\"))\n\n# Next, determine which cities are growing, shrinking, or stable in population.\n# Will need to calculate the percent population change (first pop - second pop / first pop).\n# A stable population is between -2% and 2% population change.\n# An increasing population has a greater than 2% population change.\n# A decreasing population has a less than -2% population change. \npop &lt;- pop %&gt;%\n  mutate(prc_change = ((pop20 - pop00) / pop20)) %&gt;% #calculating percent change over 2000-2020\n  mutate(change_label = ifelse(prc_change &gt; .02, \"Growing\",\n                             ifelse(prc_change &lt; -.02, \"Shrinking\", \"Stable\")))\n\nhead(pop)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\npop20\npop10\npop00\nprc_change\nchange_lable\n\n\n\n\n1900190\nAckley\n1599\n1589\n1809\n-0.131332083\nShrinking\n\n\n1900235\nAckworth\n115\n83\n85\n0.260869565\nGrowing\n\n\n1900370\nAdair\n791\n781\n839\n-0.060682680\nShrinking\n\n\n1900505\nAdel\n6153\n3682\n3435\n0.441735739\nGrowing\n\n\n1900595\nAfton\n874\n845\n917\n-0.049199085\nShrinking\n\n\n\n\n## Housing Data ##\n# Getting housing data from the 2021 5-Year American Community Survey.\n\n# Using the following API codes from the ACS:\n# total housing units = \"B25001_001\"\n# owner occupied units = \"B25003_002\"\n# renter occupied units = \"B25003_003\"\n# total occupied units = \"B25002_002\"\n# total vacant units = \"B25002_003\"\n# median house value = \"B25077_001\" \n# median house age = \"B25035_001\"\n\nhousing &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   variable = c(\"total_units\" = \"B25001_001\",\n                                \"occupied_units\" = \"B25002_002\",\n                                \"vacant_units\" = \"B25002_003\",\n                                \"owner_occupied\" = \"B25003_002\",\n                                \"renter_occupied\" = \"B25003_003\",\n                                \"median_house_value\" = \"B25007_001\"),\n                   year = 2021,\n                   output = \"wide\")\n# Getting median year built data from ACS.\nmedian_age &lt;- get_acs(geography = \"place\",\n                      state = \"IA\",\n                      variable = c(\"median_year_built\" = \"B25035_001\"),\n                      year = 2021,\n                      output = \"wide\") \n# Calculate the median house age by subtracting the median year built from 2023.\nmedian_age &lt;- median_age %&gt;%\n  mutate(median_year_builtE = ifelse(median_year_builtE == 0, NA, median_year_builtE)) %&gt;%\n  mutate(median_house_ageE = 2023 - median_year_builtE) %&gt;% \n  mutate(median_house_ageM = median_year_builtM) # Don't have to make a new moe because the subtraction didn't introduce new errors to the data.\n\n# Join the median year built data to the larger housing data frame.\nhousing &lt;- housing %&gt;% \n  left_join(median_age, by = c(\"GEOID\",\"NAME\"))\n\n# Calculate home ownership, vacany, and rental rates.\n# ALL RATES ARE PERCENTAGES with these calculations.\nhousing &lt;- housing %&gt;% \n  mutate(home_ownership_rateE = (owner_occupiedE / occupied_unitsE)) %&gt;%  # Divide owner occupied units by total occupied units.\n  mutate(home_ownership_rateM = (sqrt((owner_occupiedM^2) / (occupied_unitsE^2) + ((owner_occupiedE * occupied_unitsM)^2) / (occupied_unitsE^4)))) %&gt;%  # Calculate the new moe.\n  mutate(rental_rateE = (renter_occupiedE / occupied_unitsE)) %&gt;%  # Divide renter occupied units by total occupied units.\n  mutate(rental_rateM = (sqrt((renter_occupiedM^2) / (occupied_unitsE^2) + ((renter_occupiedE * occupied_unitsM)^2) / (occupied_unitsE^4)))) %&gt;% # Calculate the new moe.\n  mutate(vacancy_rateE = (vacant_unitsE / total_unitsE)) %&gt;%  # Divide vacant units by total units.\n  mutate(vacancy_rateM = (sqrt((vacant_unitsM^2) / (total_unitsE^2) + ((vacant_unitsE * total_unitsM)^2) / (total_unitsE^4)))) # Calculate the new moe.\n\nhead(housing)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\ntotal_unitsE\ntotal_unitsM\noccupied_unitsE\noccupied_unitsM\nvacant_unitsE\nvacant_uintsM\nowner_occupiedE\nowner_occupiedM\nrenter_occupiedE\nrenter_occupiedM\nmedian_house_valueE\nmedian_house_valueM\nmedian_year_builtE\nmedian_year_builtM\nmedian_house_ageE\nmedian_house_ageM\nhome_ownership_rateE\nhome_ownership_rateM\nvacancy_rateE\nvacancy_rateM\nrental_rateE\nrental_rateM\n\n\n\n\n1900190\nAckley\n800\n71\n705\n69\n95\n39\n553\n59\n152\n48\n705\n69\n1955\n5\n68\n5\n0.7843972\n0.11356683\n0.118750000\n0.049876190\n0.21560284\n0.07128013\n\n\n1900235\nAckworth\n47\n20\n47\n20\n0\n10\n35\n17\n12\n11\n47\n20\n1999\n11\n24\n11\n0.7446809\n0.48087923\n0.000000000\n0.212765957\n0.25531915\n0.25803094\n\n\n1900370\nAdair\n384\n60\n344\n59\n40\n27\n207\n44\n137\n44\n344\n59\n1960\n8\n63\n8\n0.6017442\n0.16435236\n0.104166667\n0.072171720\n0.39825581\n0.14500288\n\n\n1900505\nAdel\n2312\n151\n2234\n120\n78\n97\n1548\n171\n686\n157\n2234\n120\n1978\n7\n45\n7\n0.6929275\n0.08511417\n0.033737024\n0.042012837\n0.30707252\n0.07218725\n\n\n1900595\nAfton\n382\n56\n357\n53\n25\n16\n251\n40\n106\n36\n357\n53\n1962\n3\n61\n3\n0.7030812\n0.15313072\n0.065445026\n0.042969563\n0.29691877\n0.11005386\n\n\n\n\n## Taxable Property Values ##\n#link to data: https://data.iowa.gov/Local-Government-Finance/Taxable-Property-Values-in-Iowa-by-Tax-District-an/ig9g-pba5\n\ntaxable.csv &lt;- read.csv(\"C:/Users/Kailyn Hogan/OneDrive - Iowa State University/Documents/GitHub/Housing/demographics analysis/Community Profile Datasets/Taxable_Property_Values_in_Iowa_by_Tax_District_and_Year.csv\")\n# The name of the city is stored in City.Name and is in uppercase.\n# For the Census data, only the first letter of the city name is capitalized and \" city, Iowa|, Iowa\" is attached.\n# Need to lowercase taxable property values City.Name and remove \" city, Iowa|, Iowa\" from housing and population data frames before joining them. \n\nhousing &lt;- housing %&gt;% \n  mutate(NAME = str_remove(NAME,\" city, Iowa|, Iowa\"))\npop &lt;- pop %&gt;% \n  mutate(NAME = str_remove(NAME, \" city, Iowa|, Iowa\"))\ntaxable.csv &lt;- taxable.csv %&gt;% \n  mutate(City.Name = str_to_sentence(City.Name)) #str_to_sentence() uses regular sentence formatting where the first letter is capitalized.\n                              # Could have also used str_to_titletext().\n\n# Aggregate the sum of all columns and group by City.Name.\n# Function is set to sum to get the sum of all values.\n# na.rm = TRUE means the NAs get ignored when summing.\nresidential &lt;- aggregate(Residential ~ City.Name, data = taxable.csv, FUN = sum, na.rm = TRUE)\nag_land &lt;- aggregate(Ag.Land ~ City.Name, taxable.csv, FUN = sum, na.rm = TRUE)\nag_building &lt;- aggregate(Ag.Building ~ City.Name, taxable.csv, FUN = sum, na.rm = TRUE)\ncommercial &lt;- aggregate(Commercial ~ City.Name, taxable.csv, FUN = sum, na.rm = TRUE)\nindustrial &lt;- aggregate(Industrial ~ City.Name, taxable.csv, FUN = sum, na.rm = TRUE)\n# Join aggregated dataframes together to reform taxable property values data frame.\ntaxable_prop_values &lt;- residential %&gt;%\n  left_join(ag_land, by = \"City.Name\") %&gt;%\n  left_join(ag_building, by = \"City.Name\") %&gt;%\n  left_join(commercial, by = \"City.Name\") %&gt;%\n  left_join(industrial, by = \"City.Name\")\n  \nhead(taxable_property_values)\n\n\n\n\n\n\n\n\n\n\n\n\nCity.Name\nResidential\nAg.Land\nAg.Building\nCommercial\nIndustrial\n\n\n\n\n\n57582069434\n68289755861\n3756099698\n6892181005\n9990249878\n\n\nAckley\n59077903\n1756505\n61418\n20369025\n2143449\n\n\nAckworth\n10399627\n403117\n7779\n67230\n0\n\n\nAdair\n1151067\n715063\n40676\n5091781\n195604\n\n\nAdel\n257984582\n2651441\n27555\n81299747\n12757527\n\n\nAfton\n24900696\n138605\n38906\n7514181\n1583244\n\n\n\nI also download the LEHD Origin-Destination Employment Statistics (LODES) from the Census website as well. I downloaded the 2020 Residential Area Characteristics and the 2020 Workplace Area Characteristics. They are aggregated by census block. I downloaded the Geography crosswalk for IA as well. The geography is needed to associate the census block codes to their related places in Iowa."
  },
  {
    "objectID": "posts/Week_7/Week_7.html#wednesday",
    "href": "posts/Week_7/Week_7.html#wednesday",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "Wednesday",
    "text": "Wednesday\nLate on Wednesday, my team got more clarification from our boss, Chris, on what the AI Housing project actually is. From our conversation, I interpreted that our project is intended to produces a rapid, hands off, housing quality assessment for policy and grant decision makers.\n\n## Median Age of Residents ##\n\n# Getting the median age of all people in all places in Iowa from 2021 5-Year American Community Survey.\npop_age &lt;- get_acs(geography = \"place\",\n               state = \"IA\",\n               year = 2021,\n               variable = c(\"med_age\" = \"B01002_001\"),\n               output = \"wide\") %&gt;% \n  mutate(NAME = str_remove(NAME, \" city, Iowa|, Iowa\"))\n\n# Get the number of people under 18 and over 65 for all places in Iowa.\n# The age data in the American Community Survey is separated by sex, so you will need to get the data for men and then women and combine the data frames. \nmale &lt;- get_acs(geography = \"place\",\n                  state = \"IA\",\n                  year = 2021,\n                  variable = c(\"under5\" = \"B01001_003\", # Make sure to start the variable names with a letter.\n                               \"a5to9\" = \"B01001_004\",\n                               \"a10to14\" = \"B01001_005\",\n                               \"a15to17\" = \"B01001_006\",\n                               \"a65to66\" = \"B01001_020\",\n                               \"a67to69\" = \"B01001_021\",\n                               \"a70to74\" = \"B01001_022\",\n                               \"a75to79\" = \"B01001_023\",\n                               \"a80to84\" = \"B01001_024\",\n                               \"over85\" = \"B01001_025\",\n                               \"total\" = \"B01001_002\"),\n                output = \"wide\") %&gt;% \n  mutate(NAME = str_remove(NAME, \" city, Iowa|, Iowa\"))\nmale &lt;- male %&gt;% \n  mutate(under18E = under5E + a5to9E + a10to14E + a15to17E) %&gt;%\n  mutate(under18M = sqrt(under5M^2 + a5to9M^2 + a10to14M^2 + a15to17M^2)) %&gt;%\n  #mutate(prc_under18E = under18E / totalE) %&gt;% \n  #mutate(prc_under18M = moe_ratio(under18E, totalE, under18M, totalM)) %&gt;% \n  mutate(over65E = a65to66E + a67to69E + a70to74E + a75to79E + a80to84E + over85E) %&gt;%\n  mutate(over65M =  sqrt(a65to66M^2 + a67to69M^2 + a70to74M^2 + a75to79M^2 + a80to84M^2 + over85M^2)) %&gt;% \n # mutate(prc_over65E = over65E / totalE) %&gt;% \n  #mutate(prc_over65M = moe_ratio(over65E, totalE, over65M, totalM)) \n\n# Now do the same for the female age data.\nfemale &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                year = 2021,\n                variable = c(\"under5\" = \"B01001_027\",\n                             \"a5to9\" = \"B01001_028\",\n                             \"a10to14\" = \"B01001_029\",\n                             \"a15to17\" = \"B01001_030\",\n                             \"a65to66\" = \"B01001_044\",\n                             \"a67to69\" = \"B01001_045\",\n                             \"a70to74\" = \"B01001_046\",\n                             \"a75to79\" = \"B01001_047\",\n                             \"a80to84\" = \"B01001_048\",\n                             \"over85\" = \"B01001_049\",\n                             \"total\" = \"B01001_026\"),\n                output = \"wide\") %&gt;% \n  mutate(NAME = str_remove(NAME, \" city, Iowa|, Iowa\"))\nfemale &lt;- female %&gt;% \n  mutate(under18E = under5E + a5to9E + a10to14E + a15to17E) %&gt;%\n  mutate(under18M = sqrt(under5M^2 + a5to9M^2 + a10to14M^2 + a15to17M^2)) %&gt;%\n  #mutate(prc_under18E = under18E / totalE) %&gt;% \n #mutate(prc_under18M = moe_ratio(under18E, totalE, under18M, totalM)) %&gt;% \n  mutate(over65E = a65to66E + a67to69E + a70to74E + a75to79E + a80to84E + over85E) %&gt;%\n  mutate(over65M =  sqrt(a65to66M^2 + a67to69M^2 + a70to74M^2 + a75to79M^2 + a80to84M^2 + over85M^2)) %&gt;% \n  #mutate(prc_over65E = over65E / totalE) %&gt;% \n  #mutate(prc_over65M = moe_ratio(over65E, totalE, over65M, totalM)) \n\n# Combine the data frames.\nage &lt;- female %&gt;% \n  bind_rows(male)\n\n# Aggregate the data by NAME.\n# I am using a different aggregation format here by using the summary() function. \n# This will produce a separate dataframe with just under18E, under18M, over65E, over65M, prc_under18E, prc_under18M, prc_over65E, and prc_over65M.\naggregated_age &lt;- age %&gt;% \n  group_by(NAME) %&gt;% \n             summarize(under18E = sum(under18E),\n                       under18M = sqrt(sum(under18M^2)),\n                       over65E = sum(over65E),\n                       over65M = sqrt(sum(over65M)),\n                       prc_under18E = sum(under18E)/sum(totalE),\n                       prc_under18M = sqrt(sum(under18M^2) / sum(totalE)^2 + (sum(under18E)^2 * sum(totalM^2)) / sum(totalE)^4),\n                       prc_over65E = sum(over65E)/sum(totalE),\n                       prc_over65M = sqrt(sum(over65M^2) / sum(totalE)^2 + (sum(over65E)^2 * sum(totalM^2)) / sum(totalE)^4))\n\n# Add a column that states whether a place is \"aged\", \"stable\" or \"young.\"\n# Using a 2% difference to gauge a stable population.\n# Aged population has a less than -2% difference between the percent under 18 and the percent over 65.\n# Young population has a greater than 2% difference between the percent under 18 and the percent over 65. \naggregated_age &lt;- aggregated_age %&gt;%\n  mutate(age_label = ifelse(prc_under18E - prc_over65E &gt; .02, \"Young\",\n                               ifelse(prc_under18E - prc_over65E &lt; -.02, \"Aging\", \"Stable\")))\n\n## Add the age data to the population dataframe.\npop &lt;- pop %&gt;% \n  left_join(aggregated_age,by = c(\"NAME\")) %&gt;% \n  left_join(pop_age, by = c(\"GEOID\",\"NAME\"))\n\nhead(aggregated_age)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAME\nunder18E\nunder18M\nover65E\nover65M\nprc_under18E\nprc_under18M\nprc_over65E\nprc_over65M\nage_label\n\n\n\n\nAckley\n402\n68.39591\n349\n9.030714\n0.23563892\n0.044036378\n0.20457210\n0.0166781814\nYoung\n\n\nAckworth\n50\n26.47640\n24\n6.256889\n0.33783784\n0.212313083\n0.16216216\n0.0692779288\nYoung\n\n\nAdair\n206\n67.47592\n144\n7.692109\n0.24065421\n0.088939653\n0.16822430\n0.0301622637\nYoung\n\n\nAdel\n1708\n285.81987\n661\n14.821549\n0.29111982\n0.050896612\n0.11266405\n0.0062375436\nYoung\n\n\nAfton\n208\n60.94260\n189\n8.606830\n0.22757112\n0.073795265\n0.20678337\n0.0302370669\nYoung\n\n\n\n\nlibrary(tidyverse)\n\n## Unemployment ##\n# Getting unemployment data from the 2021 5-Year American Community Survey.\n\n# Get the total number of people in the workforce and the total number of the labor force that is unemployed.\n# Calculate the percent of the workforce that is unemployed. \nunemployment &lt;- get_acs(state = \"IA\", \n                   geography = \"place\",\n                   year = 2021,\n                   variable = c(\"total_workforce\" = \"B23025_003\",\n                                \"unemployed\" = \"B23025_005\"),\n                   output = \"wide\") %&gt;% \n  mutate(prc_unemployed = unemployedE / total_workforceE,\n         unemployed_moe = moe_ratio(unemployedE, total_workforceE, unemployedM, total_workforceM)) %&gt;% \n  mutate(NAME = str_remove(NAME,\" city, Iowa|, Iowa\"))\n\nhead(unemployment)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\ntotal_workforceE\ntotal_workforceM\nunemployedE\nunemployedM\nprc_unemplyed\nunemployed_moe\n\n\n\n\n1900190\nAckley\n828\n88\n25\n17\n0.030193237\n0.020780658\n\n\n1900235\nAckworth\n73\n39\n6\n9\n0.082191781\n0.130873976\n\n\n1900370\nAdair\n440\n101\n25\n25\n0.056818182\n0.058295873\n\n\n1900505\nAdel\n3125\n245\n127\n93\n0.040640000\n0.029930074\n\n\n1900595\nAfton\n480\n100\n42\n31\n0.087500000\n0.067106702\n\n\n\n\n## Commuting ##\nlibrary(tidyverse)\nlibrary(tidycensus)\n\n# Getting commute data from the 2021 5-Year American Community Survey.\n\n# Get the total number of people available for commute data and the total number of people commuting to work. \n# Calculate the percent of people commuting to work. \nprc_travel &lt;- get_acs(geography = \"place\",\n                     state = \"IA\",\n                     variable = c(\"total\" = \"B08008_001\",\n                                  \"travel\" = \"B08008_004\"),\n                     year = 2021,\n                     output = \"wide\") %&gt;% \n  mutate(prc_travel = travelE / totalE,\n         travel_moe = (moe_ratio(travelE, totalE, travelM, totalM))) %&gt;% \n  mutate(NAME = str_remove(NAME, \" city, Iowa|, Iowa\"))\n\nhead(prc_travel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\ntotalE\ntotalM\ntravelE\ntravelM\nprc_travel\ntravel_moe\n\n\n\n\n1900190\nAckley\n797\n85\n534\n83\n0.6700125\n0.12629855\n\n\n1900235\nAckworth\n67\n38\n64\n39\n0.9552239\n0.79519912\n\n\n1900370\nAdair\n412\n93\n230\n57\n0.5582524\n0.18713615\n\n\n1900505\nAdel\n2924\n229\n2216\n263\n0.7578659\n0.10776390\n\n\n1900595\nAfton\n434\n90\n319\n77\n0.7350230\n0.23390328\n\n\n\nWhen I was working on the demographic analysis on Wednesday, I took a break to do some more research on demographic analyses in general. I found this link on my project leader’s, Liesl Eathington, ISU webpage.\nThe link is to a local news article that Liesl was interviewed for talking about the characteristics of thriving rural, small towns. The article also talked about some demographic analysis that was done on rural communities for grant purposes. Through this analysis, the researchers discovered that it was key to analyze housing, income, and jobs in a community to understand it. The researchers also found that an indicator of a declining small town is deteriorating infrastructure like streets and sewers.\nSmall towns that are thriving have one or more of the following characteristics:\n\nclose healthcare\nlocal day care options\nlocal grocery store\nlocal utilities\nlocal restaurants\nlocal shops\nlocal school(s)\n\nKnowing this information will help me finish the demographic analysis of Iowa’s communities. I want to see if I can find available data on infrastructure conditions to identify communities potentially in need of our AI Model."
  },
  {
    "objectID": "posts/Week_7/Week_7.html#thursday",
    "href": "posts/Week_7/Week_7.html#thursday",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "Thursday",
    "text": "Thursday\nMy group spent all morning working on our team’s teaser video. I worked on this blog and this week’s team blog when I had breaks from that. The team blog can be found on this blog and on the 2023 DSPG blog.\nI compiled the data I currently have for the demographic analysis into a larger data frame called “iowa.” I used the function write.csv() to create a CSV file from the “iowa” dataframe. This still needs updated because I am not done working with the WAC and RAC data, but it is a starting point. Later, I will be putting this CSV file into Tableau to visualize the data.\n\nlibrary(tidyverse)\n\n\niowa # Name of aggregated database.\n\niowa &lt;- pop %&gt;% \n  left_join(housing, by = c(\"GEOID\",\"NAME\")) %&gt;% \n  left_join(taxable_prop_values, by = \"NAME\") %&gt;% \n  left_join(unemployment, by = c(\"GEOID\",\"NAME\")) %&gt;% \n  left_join(prc_travel, by = c(\"GEOID\",\"NAME\"))\n\n# Export as CSV.\nwrite.csv(iowa, \"analysing_iowa_communities.csv\", row.names = FALSE)\n\n# Read the CSV file. \ndata &lt;- read.csv(\"C:/Users/Kailyn Hogan/OneDrive - Iowa State University/Documents/GitHub/Housing/demographics analysis/analysing_iowa_communities.csv\") \n\n# Display the contents of the CSV file.\nknitr::kable(head(arrange(data, NAME)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\npop20\npop10\npop00\nprc_change\nchange_label\nunder18E\nunder18M\nover65E\nover65M\nprc_under18E\nprc_under18M\nprc_over65E\nprc_over65M\nage_label\nmed_ageE\nmed_ageM\ntotal_unitsE\ntotal_unitsM\noccupied_unitsE\noccupied_unitsM\nvacant_unitsE\nvacant_unitsM\nowner_occupiedE\nowner_occupiedM\nrenter_occupiedE\nrenter_occupiedM\nmedian_house_valueE\nmedian_house_valueM\nmedian_year_builtE\nmedian_year_builtM\nmedian_house_ageE\nmedian_house_ageM\nhome_ownership_rateE\nhome_ownership_rateM\nvacancy_rateE\nvacancy_rateM\nrental_rateE\nrental_rateM\nResidential\nAg.Land\nAg.Building\nCommercial\nIndustrial\ntotal_workforceE\ntotal_workforceM\nunemployedE\nunemployedM\nprc_unemployed\nunemployed_moe\ntotalE\ntotalM\ntravelE\ntravelM\nprc_travel\ntravel_moe\n\n\n\n\n1900190\nAckley\n1599\n1589\n1809\n-0.1313321\nShrinking\n402\n68.39591\n349\n9.030714\n0.2356389\n0.0440364\n0.2045721\n0.0166782\nYoung\n41.5\n3.0\n800\n71\n705\n69\n95\n39\n553\n59\n152\n48\n705\n69\n1955\n5\n68\n5\n0.7843972\n0.1135668\n0.1187500\n0.0498762\n0.2156028\n0.0712801\n59077903\n1756505\n61418\n20369025\n2143449\n828\n88\n25\n17\n0.0301932\n0.0207807\n797\n85\n534\n83\n0.6700125\n0.1262985\n\n\n1900235\nAckworth\n115\n83\n85\n0.2608696\nGrowing\n50\n26.47640\n24\n6.256889\n0.3378378\n0.2123131\n0.1621622\n0.0692779\nYoung\n38.1\n30.8\n47\n20\n47\n20\n0\n10\n35\n17\n12\n11\n47\n20\n1999\n11\n24\n11\n0.7446809\n0.4808792\n0.0000000\n0.2127660\n0.2553191\n0.2580309\n10399627\n403117\n7779\n67230\n0\n73\n39\n6\n9\n0.0821918\n0.1308740\n67\n38\n64\n39\n0.9552239\n0.7951991\n\n\n1900370\nAdair\n791\n781\n839\n-0.0606827\nShrinking\n206\n67.47592\n144\n7.692109\n0.2406542\n0.0889397\n0.1682243\n0.0301623\nYoung\n37.7\n8.6\n384\n60\n344\n59\n40\n27\n207\n44\n137\n44\n344\n59\n1960\n8\n63\n8\n0.6017442\n0.1643524\n0.1041667\n0.0721717\n0.3982558\n0.1450029\n1151067\n715063\n40676\n5091781\n195604\n440\n101\n25\n25\n0.0568182\n0.0582959\n412\n93\n230\n57\n0.5582524\n0.1871361\n\n\n1900505\nAdel\n6153\n3682\n3435\n0.4417357\nGrowing\n1708\n285.81987\n661\n14.821549\n0.2911198\n0.0508966\n0.1126641\n0.0062375\nYoung\n36.8\n1.5\n2312\n151\n2234\n120\n78\n97\n1548\n171\n686\n157\n2234\n120\n1978\n7\n45\n7\n0.6929275\n0.0851142\n0.0337370\n0.0420128\n0.3070725\n0.0721873\n257984582\n2651441\n27555\n81299747\n12757527\n3125\n245\n127\n93\n0.0406400\n0.0299301\n2924\n229\n2216\n263\n0.7578659\n0.1077639\n\n\n1900595\nAfton\n874\n845\n917\n-0.0491991\nShrinking\n208\n60.94260\n189\n8.606830\n0.2275711\n0.0737953\n0.2067834\n0.0302371\nYoung\n39.0\n9.2\n382\n56\n357\n53\n25\n16\n251\n40\n106\n36\n357\n53\n1962\n3\n61\n3\n0.7030812\n0.1531307\n0.0654450\n0.0429696\n0.2969188\n0.1100539\n24900696\n138605\n38906\n7514181\n1583244\n480\n100\n42\n31\n0.0875000\n0.0671067\n434\n90\n319\n77\n0.7350230\n0.2339033\n\n\n1900640\nAgency\n620\n638\n622\n-0.0032258\nStable\n71\n29.79933\n123\n6.383032\n0.1501057\n0.0678777\n0.2600423\n0.0458014\nAging\n49.6\n10.8\n233\n41\n211\n38\n22\n22\n154\n29\n57\n25\n211\n38\n1959\n6\n64\n6\n0.7298578\n0.1901772\n0.0944206\n0.0958713\n0.2701422\n0.1280830\n82024799\n119286\n70\n2742290\n2872782\n240\n63\n7\n7\n0.0291667\n0.0301548\n232\n63\n219\n63\n0.9439655\n0.3734276"
  },
  {
    "objectID": "posts/Week_7/Week_7.html#friday",
    "href": "posts/Week_7/Week_7.html#friday",
    "title": "Week Seven of Data Science for the Public Good",
    "section": "Friday",
    "text": "Friday\nOn Friday, I gave the weekly wrap up presentation."
  },
  {
    "objectID": "posts/Week_7/Team_Blog_Week_7.html",
    "href": "posts/Week_7/Team_Blog_Week_7.html",
    "title": "Team Blog: Week Seven",
    "section": "",
    "text": "For more detailed information on what each member of the housing team has accomplished thus far, check out their individual blog pages. Links are embedded in their specific sections."
  },
  {
    "objectID": "posts/Week_7/Team_Blog_Week_7.html#updates-this-week",
    "href": "posts/Week_7/Team_Blog_Week_7.html#updates-this-week",
    "title": "Week Seven Team Blog",
    "section": "",
    "text": "** include teaser video if completed **"
  },
  {
    "objectID": "posts/Week_7/Team_Blog_Week_7.html#final-presentation-outline",
    "href": "posts/Week_7/Team_Blog_Week_7.html#final-presentation-outline",
    "title": "Team Blog: Week Seven",
    "section": "Final Presentation Outline",
    "text": "Final Presentation Outline\n\n\n\n\n\n\n\n\nSPEAKER\nSECTION\nTIME\n\n\nMorenike\n Introduction\n\nOverview of the project objectives and goals\nImportance of data collection and analysis in real estate and community analysis\n\n2 min\n\n\nAngelina and Kailyn\nData Collection\nScraping from Beacon and Vanguard\n\nBrief explanation of Beacon and Vanguard as data sources\nScraping process\nImportance of obtaining accurate and reliable data from these sources\n\nAddress Cleaning and Google Links\n\nAddress cleaning and standardization techniques\nUtilizing Google links for Google Street View image retrieval\nReasons for having accurate and complete address data\nScraping from Google Street View\nExtracting street view images for AI model use\nTechniques used for automated data extraction from Google Street View\n\nScraping from Zillow\n\nIntroduction to Zillow as a real estate data source\nExtracting relevant property data from Zillow\nChallenges and limitations of scraping from Zillow\n\n5 min\n\n\nGavin\nAngelina and Kailyn on Manual Image Sorting\nAI Model Creation\nOnly displaying images for one of the models. Note in presentation the ones that do exist, but we are using only one model to refine presentation.\nManual Image Sorting to Train Models\n\nTechniques used for organizing and categorizing the obtained images\nPurpose of image sorting\n\nBuilding AI Models (Binary and Multiple)\n\nKeras and Tensorflow Libraries\nRefining image data\nAI has to be able to easily read the images\nIdentify Labels (Binary vs. Multiple)\nSplit Training, Testing, and Evaluating Data\nBuild Layers\nTrain the AI Models\nEvaluate\nExport\n\n10 min\n\n\nGavin\nThe Thing Gavin Made that Writes to a CSV\n8 min\n\n\nKailyn\nDemographic Analysis and Profiling\nSelect Communities\n\nKey findings and insights regarding population, age groups, etc.\n\nAll Iowa Communities\n\nOverview of the demographic analysis methodology\nComparative analysis of Iowa communities based on demographic factors\nVisualization techniques used to present the analysis results\n\n6 min\n\n\nAngelina\nVisualizing Housing Quality Data from AI Model Outputs (with GIS?)\nGeocoding Addresses\n\nExplanation of geocoding process and its importance\nComparison of geocoding in R\nEvaluation of advantages and limitations of each geocoding approach\n\nMapping AI Model data\n\nApplication of spatial data for visualizing and analyzing house quality\nTechniques used for representing house quality on maps\nInterpretation and insights gained from geographical Analysis of house quality\n\n6 min\n\n\nMorenike\nConclusion\n\nSummary of the project workflow and key findings\nLessons learned and challenges encountered\nFuture possibilities and areas for further improvement\n\n2 min\n\n\n\nQuestions\n10 min"
  },
  {
    "objectID": "posts/Week_7/Team_Blog_Week_7.html#week-seven-for-the-housing-team",
    "href": "posts/Week_7/Team_Blog_Week_7.html#week-seven-for-the-housing-team",
    "title": "Team Blog: Week Seven",
    "section": "",
    "text": "For more detailed information on what each member of the housing team has accomplished thus far, check out their individual blog pages. Links are embedded in their specific sections."
  }
]