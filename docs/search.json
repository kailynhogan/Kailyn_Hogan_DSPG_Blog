[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Kailyn Hogan is from Delhi, Iowa, and is a senior at Iowa State University majoring in Community and Regional Planning with a minor in Geographic Information Systems. She is also Vice President of the undergraduate Community and Regional Planning Club at Iowa State. Kailyn believes combining data science and urban planning is vital in today’s data-driven world. After college, Kailyn hopes to join the Peace Corps to continue aiding communities and diversifying her cultural education."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kailyn Hogan’s DSPG Blog",
    "section": "",
    "text": "Week Five of Data Science for the Public Good\n\n\n\n\n\n\n\nWeek Five\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nTeam Blog: Week Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four of Data Science for the Public Good\n\n\nGathering addresses and images for our AI Models!\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nCollecting Address Data in the United States\n\n\n\n\n\n\n\nLittle Things\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three of Data Science for the Public Good\n\n\nI gave a Morning Coffee Talk!\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Two of Data Science for the Public Good\n\n\nRelearning TidyCensus in Vegas!\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek One of Data Science for the Public Good\n\n\nDataCamp! DataCamp! DataCamp!\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Little Things/WaysToGetAddresses.html",
    "href": "posts/Little Things/WaysToGetAddresses.html",
    "title": "Collecting Address Data in the United States",
    "section": "",
    "text": "Address Data Collection\n\nthe .csv file that Liesl found early on that holds all of the addresses in the United States\nSpatial address collection from Beacon and Vanguard GIS data"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#project-overview",
    "href": "posts/Team_Blog_Week4/Team Blog.html#project-overview",
    "title": "Team Blog: Week Four",
    "section": "Project Overview",
    "text": "Project Overview\nThis is the project plan we came up with the first week of DSPG. This project is intended to span over three years with DPSG, and different interns will be working on it in the coming years. Thus, the project plan is ambitious for this summer."
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#problem-statement",
    "href": "posts/Team_Blog_Week4/Team Blog.html#problem-statement",
    "title": "Team Blog: Week Four",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe absence of a comprehensive and unbiased assessment of housing quality in rural communities poses challenges in identifying financing gaps and effectively allocating resources for housing improvement. Consequently, this hinders the overall well-being and health of residents, impacts workforce stability, diminishes rural vitality, and undermines the economic growth of Iowa. Moreover, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations further compound the problem. To address these challenges, there is a pressing need for an AI-driven approach that can provide a more accurate and objective evaluation of housing quality, identify financing gaps, and optimize the allocation of local, state, and federal funds to maximize community benefits.\nUtilizing web scraping techniques to collect images of houses from various assessor websites, an AI model can be developed to analyze and categorize housing features into good or poor quality. This can enable targeted investment strategies. It allows for the identification of houses in need of improvement and determines the areas where financial resources should be directed. By leveraging AI technology in this manner, the project seeks to streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#goals-and-objectives",
    "href": "posts/Team_Blog_Week4/Team Blog.html#goals-and-objectives",
    "title": "Team Blog: Week Four",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nImage Gathering\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVangaurd: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center\n\n\nBuild, Train, and Test AI Models\n\nVegetation Model\nSiding Model\nGutter Model\n\nCreate Database of Housing Information\n\nZillow\nRealtors.com\nCounty Assessor Pages\n\nVanguard: Independence\nBeacon Schneider: Slater, New Hampton, and Grundy Center"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#our-progress",
    "href": "posts/Team_Blog_Week4/Team Blog.html#our-progress",
    "title": "Team Blog: Week Four",
    "section": "Our Progress",
    "text": "Our Progress\nWe have been making good progress to complete the goals and objectives we outlined above. Since the beginning of the Data Science for the Public Good Program, we have been expanding our knowledge of data science, particularly in areas that relate to this housing project. We have been learning and covering new concepts through Data Camp. We have also watched two webinars on TidyCensus training, as well as started creating AI Models to practice with.\n\nData Camp Training:\n\nGitHub Concepts\nAI Fundamentals\nIntroduction to R\nIntermediate R\nIntroduction to the Tidyverse\nWeb Scraping in R\nIntroduction to Deep Learning with Keras\n\n\n\nTidyCensus Demographic Data Collection:\nOne of the first steps in our project was to explore the available demographic data in our selected cities and counties. We thought it valuable to understand the demographic data, and we have represented in the plots below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Test AI Models:\nThe next step was creating an AI Model. We decided to create an AI Model early in the project before finishing the housing data collection so that we had a better understanding when it came to putting everything together. The AI Model below tests for vegetation in front of houses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Week:\n\nIn-Person Data Collection\nOn Tuesday this week, the entire DSPG program went to Slater to practice data collection in person. The housing group took this as an opportunity to collect some housing photos on the ground to use in our AI Model later on.\n\n\nGoogle Street View and URLs\nWe are getting the majority of our photos for the AI to use from Google Street View. Google has an API key that you can use to generate an image for a specific address. We spent the first half of this week pulling addresses from each of our cities and creating URLs to pull the images from Google Street View.\nWe ran into a couple of problems when doing this, the biggest of which is displayed in the images below. Because we are working with cities in rural areas, there is not Google Street View images available for every street in our cities.\n\n\n\nGoogle Street View information for Grundy Center, Iowa. For reference, population was 2,811 as of 2023.\n\n\n\n\n\nGoogle Street View information for Slater, Iowa. For reference, population was 1,639 as of 2023.\n\n\n\n\n\nGoogle Street View information for Independence, Iowa. For reference, population was 6,307 as of 2023.\n\n\n\n\n\nGoogle Street View information for New Hampton, Iowa. For reference, population was 3,368 as of 2023.\n\n\nBelow is a sample from the tables we created containing the URLs to grab the images from Google Street View.\n\n\n\n\nWeb Scraping\nOnce we were finished collecting addresses and generating URLs, we moved on to scraping the web for more images. We decided to grab images from Zillow, Realtors.com, and the County Assessor pages for our cities. We were able to successfully scrape images from Zillow this week.\n\n\n\n\n\n\n\n\n\n\nWhen web scraping, we ran into a problem with blurred houses. Upon some research, we found out that some home owners pay Google to have their home blurred for Google Street View.\n\n\n\n\n\nThe next websites we are scraping hold the Iowa Assessors housing data for our four cities. We found a webpage that has links to every counties assessor website. The key at the bottom shows where the data is held. Yellow means the data is held by Vanguard. Blue means the data is online. For most of Iowa’s counties, this means that it is held by Beacon Schneider.\n\n\n\n\n\n\n\n\nWeb Scraping Issues\nIndependence had issues where the house number was listed as 100/101 and also had 100 1/2. Thankfully the function to grab the Google images ran all the way but said there were 50 errors including both addresses with two house numbers and with half signs. If you remove one of the address numbers or remove the 1/2 (basically removing the / sign) the image url still brings you to a Google image. We could possibly go back through and grab these URL’s and alter them to try and grab these addresses if necessary.\n\nHappies\n\nhad a great meeting with Erin Olson-Douglas\nfinished collecting and creating URL addresses for Google Street View images\nZillow owns Trulia so we don’t have to web scrape both sites :)\nable to successfully scrape some things from Zillow !\n\n\n\nCrappies\n\nWeb Scraping\nBeacon and Vanguard have anti-web scraping protections\nAngelina’s Excel is stupid"
  },
  {
    "objectID": "posts/Team_Blog_Week4/Team Blog.html#future-plans-and-next-steps",
    "href": "posts/Team_Blog_Week4/Team Blog.html#future-plans-and-next-steps",
    "title": "Team Blog: Week Four",
    "section": "Future Plans and Next Steps",
    "text": "Future Plans and Next Steps\nOnce we are able to scrape enough images off of Zillow, Realtors.com, and the assessor pages, we will be able to move on with creating AI Models. The diagram below outlines how the AI Models will work in the next steps of out project."
  },
  {
    "objectID": "posts/Week_1/Week_1.html",
    "href": "posts/Week_1/Week_1.html",
    "title": "Week One of Data Science for the Public Good",
    "section": "",
    "text": "The priority for week one of Data Science for the Public Good was DataCamp training. I completed the following list of DataCamp courses this week."
  },
  {
    "objectID": "posts/Week_1/Week_1.html#datacamp-trainings",
    "href": "posts/Week_1/Week_1.html#datacamp-trainings",
    "title": "Week One of Data Science for the Public Good",
    "section": "DataCamp Trainings",
    "text": "DataCamp Trainings\n\nAI Fundamentals\nGitHub Concepts\nR Programming Assessment\nUnderstanding and Interpreting Data Assessment\nIntroduction to R\nIntroduction to the Tidyverse"
  },
  {
    "objectID": "posts/Week_2/Week_2.html",
    "href": "posts/Week_2/Week_2.html",
    "title": "Week Two",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa &lt;- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %&gt;% \n  mutate(tenure = \"Owner\") %&gt;% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa &lt;- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %&gt;% \n  mutate(tenure = \"Renter\") %&gt;% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa &lt;- rent_iowa %&gt;% \n  bind_rows(own_iowa)%&gt;% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage &lt;- c(\"medage\" = \"B01002_001\")\n\ngrundy &lt;- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia &lt;- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 &lt;- grundy %&gt;% \n  bind_rows(ia,inde, new) %&gt;% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign &lt;- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %&gt;% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign &lt;- foreign %&gt;% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs &lt;- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house &lt;- grundy_acs %&gt;% \n  bind_rows(inde_acs,new_acs)\nmed_house %&gt;%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un &lt;- grundy_un %&gt;% \n  bind_rows(inde_un,new_un)\npct_un %&gt;%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders.\ngit_add\ngit_commit\ngit_push"
  },
  {
    "objectID": "posts/Week_2/Week_2.html#tidycensus-practice",
    "href": "posts/Week_2/Week_2.html#tidycensus-practice",
    "title": "Week Two",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa &lt;- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %&gt;% \n  mutate(tenure = \"Owner\") %&gt;% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa &lt;- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %&gt;% \n  mutate(tenure = \"Renter\") %&gt;% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa &lt;- rent_iowa %&gt;% \n  bind_rows(own_iowa)%&gt;% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %&gt;% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage &lt;- c(\"medage\" = \"B01002_001\")\n\ngrundy &lt;- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew &lt;- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia &lt;- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %&gt;% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 &lt;- grundy %&gt;% \n  bind_rows(ia,inde, new) %&gt;% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign &lt;- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign &lt;- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %&gt;% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign &lt;- foreign %&gt;% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %&gt;% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs &lt;- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house &lt;- grundy_acs %&gt;% \n  bind_rows(inde_acs,new_acs)\nmed_house %&gt;%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un &lt;- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %&gt;% \n  filter(NAME == \"Grundy Center city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"Independence city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un &lt;- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %&gt;% \n  filter(NAME == \"New Hampton city, Iowa\") %&gt;% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un &lt;- grundy_un %&gt;% \n  bind_rows(inde_un,new_un)\npct_un %&gt;%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders.\ngit_add\ngit_commit\ngit_push"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html",
    "href": "posts/Week_3/Week_Three.html",
    "title": "Week Three of Data Science for the Public Good",
    "section": "",
    "text": "This week my team has been trying to gather resources to use in our AI model. We are trying to scrape data from multiple different sources including Beacon, Vanguard, and Trulia to compile photos and train our AI model.\nI have been learning how to web scrape, and I have completed the following DataCamp trainings:"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#datacamp-trainings",
    "href": "posts/Week_3/Week_Three.html#datacamp-trainings",
    "title": "Week Three of Data Science for the Public Good",
    "section": "DataCamp Trainings",
    "text": "DataCamp Trainings\n\nIntermediate R\nWeb Scraping in R\n\nI have also created an R Markdown file to document my web scraping practice.\nI did a quick Google search for a web scraper for Beacon before I attempted it myself, and I found a GitHub page dedicated to one:\nhttps://github.com/openaddresses/machine/issues/580\nI am not sure it is relevant to the data I am trying to scrape from Beacon.\nI also learned how to create a quarto blog this week!"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#ai-modeling",
    "href": "posts/Week_3/Week_Three.html#ai-modeling",
    "title": "Week Three of Data Science for the Public Good",
    "section": "AI Modeling",
    "text": "AI Modeling\nTo better understand the AI model my group is trying to create, I am watching the following YouTube videos:\n\nhttps://www.youtube.com/watch?v=19LQRx78QVU&list=PLgNJO2hghbmiXg5d4X8DURJP9yv9pgjIu&index=1&ab_channel=NicholasRenotte\nhttps://www.youtube.com/watch?v=jztwpsIzEGc&ab_channel=NicholasRenotte"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html#morning-coffee-talk",
    "href": "posts/Week_3/Week_Three.html#morning-coffee-talk",
    "title": "Week Three of Data Science for the Public Good",
    "section": "Morning Coffee Talk",
    "text": "Morning Coffee Talk\nI also gave the Morning Coffee Talk on Thursday this week over the Des Moines Housing Project I was a part of. The Des Moines Housing Project was conducted by czb, a firm located in Bath, Maine. I was hired as a student researcher for them this past spring, and I conducted housing surveys on roughly 6,000 properties in Southwestern Des Moines."
  },
  {
    "objectID": "posts/Week_5/Week_5.html",
    "href": "posts/Week_5/Week_5.html",
    "title": "Week Five of Data Science for the Public Good",
    "section": "",
    "text": "Chris informed us this morning that we need to include a demographic analysis or report in our project. We reviewed the initial project brief, and I realized that my group had skipped to the end of Year 1 when we started with AI Models. Whoops!\nWe need to go back at some point and work with demographics. I started a little bit of it today. We need to include the following:\n\nIdentify communities with populations between 500 and 10,000\nChange in population\nPresence of schools\nMean age of residents (I wonder if we should discuss changing this to median because it is less influenced by outliers)\nIndustry report\nAg Census data:\n\nNumber of Operators\nOperator Owned\netc.\n\nHousing appreciation and depreciation vs. inflation\n\nWe also started sorting the Google Street View images to train our AI Models on Monday. We need to sort based on six different models. We started with our first three.\n\nIs a house present?\n\nYes\nNo\n\nIs it a clear image of a house?\n\nObstructed\nPartially obstructed\nNot obstructed\n\nAre there multiple houses?\n\nOne House\nMore than one house\n\n\nWhen we started sorting the images, I noticed an error with an Independence image. The image on the left below is the one that we downloaded from Google Street View. Because I have been in Independence before, I could tell that this was not a photo of Independence. The photo on the left is from a different place, and the photo on the right is the same address but actually in Independence.\nI checked the URL we used to generate the Google Street View image on the left, and I noticed that we did not specify the city and state. In fact, we didn’t specify the city and state for any of the Independence URLs or the New Hampton URLs. Thankfully this was a quick fix. We just added the city and state to the URL files, and the images downloaded quickly.\n\n\n\nError in Independence Address for Google Image API\n\n\nWhile I was waiting for images to download on Monday, I started working on scraping Realtor.com. NOTE: it is Realtor.com not Realtors.com. I definitely have been misspelling it. From the brief look I took at web scraping Realtor.com, it looks like it might be slightly more complicated than Zillow.\nThe address data is stored differently on Realtor.com, and I was not successful in scraping it. Instead of being on one line of HTML, it is split up on multiple.\n\n\n\nRealtor.com address HTML"
  },
  {
    "objectID": "posts/Week_5/Week_5.html#monday",
    "href": "posts/Week_5/Week_5.html#monday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "",
    "text": "Chris informed us this morning that we need to include a demographic analysis or report in our project. We reviewed the initial project brief, and I realized that my group had skipped to the end of Year 1 when we started with AI Models. Whoops!\nWe need to go back at some point and work with demographics. I started a little bit of it today. We need to include the following:\n\nIdentify communities with populations between 500 and 10,000\nChange in population\nPresence of schools\nMean age of residents (I wonder if we should discuss changing this to median because it is less influenced by outliers)\nIndustry report\nAg Census data:\n\nNumber of Operators\nOperator Owned\netc.\n\nHousing appreciation and depreciation vs. inflation\n\nWe also started sorting the Google Street View images to train our AI Models on Monday. We need to sort based on six different models. We started with our first three.\n\nIs a house present?\n\nYes\nNo\n\nIs it a clear image of a house?\n\nObstructed\nPartially obstructed\nNot obstructed\n\nAre there multiple houses?\n\nOne House\nMore than one house\n\n\nWhen we started sorting the images, I noticed an error with an Independence image. The image on the left below is the one that we downloaded from Google Street View. Because I have been in Independence before, I could tell that this was not a photo of Independence. The photo on the left is from a different place, and the photo on the right is the same address but actually in Independence.\nI checked the URL we used to generate the Google Street View image on the left, and I noticed that we did not specify the city and state. In fact, we didn’t specify the city and state for any of the Independence URLs or the New Hampton URLs. Thankfully this was a quick fix. We just added the city and state to the URL files, and the images downloaded quickly.\n\n\n\nError in Independence Address for Google Image API\n\n\nWhile I was waiting for images to download on Monday, I started working on scraping Realtor.com. NOTE: it is Realtor.com not Realtors.com. I definitely have been misspelling it. From the brief look I took at web scraping Realtor.com, it looks like it might be slightly more complicated than Zillow.\nThe address data is stored differently on Realtor.com, and I was not successful in scraping it. Instead of being on one line of HTML, it is split up on multiple.\n\n\n\nRealtor.com address HTML"
  },
  {
    "objectID": "posts/Week_5/Week_5.html#tuesday",
    "href": "posts/Week_5/Week_5.html#tuesday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Tuesday",
    "text": "Tuesday\nOn Tuesday, we continued sorting images to train our AI Models. I finished sorting the Slater images for a clear view of the house yesterday, and today I worked on the same sorting for Grundy Center.\nWe need about 200 images for each category to train our model. We are struggling to find enough “bad” images.\nChris came over to talk to us about the app he is creating for the Housing Team. His app is meant to make image sorting easier, so he wants to be able to get it to us as soon as possible. He needed a database created with the house address, Google Street View image URL, and city to finish the app. I created the housing database for all the addresses for Slater, Independence, Grundy Center, and New Hampton.\n\n\n\n\n\nChris also suggested that Angelina and I take a look at spatial graphing in R. He said to try the DataCamp course Geospatial in R, and he said to look up Kyle Walkers TidyCensus book.\nOn Tuesday, I also worked more on the demographics analysis that Chris asked for. I am starting the analysis by looking at total population in Iowa’s communities. The first plot I created plotted the change in population for Iowa as a whole.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidycensus)\nlibrary(ggthemes)\n\n# renaming the variables now so I don't have to do it later\npop00 &lt;- c(\"pop\" = \"P001001\")\npop10 &lt;- c(\"pop\" = \"P001001\")\npop20 &lt;- c(\"pop\" = \"P1_001N\")\n\n# iowa 2000 population\niowa00 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        year = 2000,\n                        output = \"wide\",\n                        variable = pop00) %&gt;% \n  mutate(year = 2000)\n\nGetting data from the 2000 decennial Census\nUsing Census Summary File 1\n\n# iowa 2010 population\niowa10 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        variable = pop10,\n                        year = 2010,\n                        output = \"wide\") %&gt;% \n  mutate(year = 2010)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n# iowa 2020 population\niowa20 &lt;- get_decennial(geography = \"state\",\n                        state = \"IA\",\n                        variable = pop20,\n                        year = 2020,\n                        output = \"wide\") %&gt;% \n  mutate(year = 2020)\n\nGetting data from the 2020 decennial Census\nUsing the PL 94-171 Redistricting Data summary file\nNote: 2020 decennial Census data use differential privacy, a technique that\nintroduces errors into data to preserve respondent confidentiality.\ni Small counts should be interpreted with caution.\ni See https://www.census.gov/library/fact-sheets/2021/protecting-the-confidentiality-of-the-2020-census-redistricting-data.html for additional guidance.\n\n# bind 2000-2020 data together\niowa &lt;- iowa20 %&gt;% \n  bind_rows(iowa10,iowa00)\n\n# plot it\nchange_pop_iowa.jpg &lt;- iowa %&gt;% \n  ggplot(aes(x = year, y = pop))+\n  geom_line()+\n  geom_point(size = 2)+\n  geom_text(aes(label = scales::comma(pop)), hjust = -.25)+  \n  scale_y_continuous(label = scales::comma)+  # how do I change the size of the axis labels?\n  scale_x_continuous(limits = c(1998, 2025),\n                     breaks = c(2000,2010,2020))+\n  theme_fivethirtyeight() +  \n  theme(legend.position = \"bottom\")+\n  labs(title = \"Change in Total Population\",\n       subtitle = \"State of Iowa\",\n       y = \"Population\",\n       x = \"\",\n       color = \"\", \n       caption = \"2000-2020 Decennial Census\")\n\n# save the plot as a .jpg\nchange_pop_iowa.jpg %&gt;% ggsave(filename = \"change_pop_iowa.jpg\",width = 6,height = 6, dpi = 400)\n\n\n\n\n\n\nNext, I pulled the total population for each individual community in Iowa for 2020, 2010, and 2000. Using the 2020 Decennial Census data, I found that there are 408 communities in Iowa that fall within our population parameters. This is 39.69% of all cities in Iowa.\nI also looked at the growth rate for communities in Iowa. Because most of Iowa’s cities are on the smaller size, I don’t think that population is an effective indicator for this project. In fact, the median population for communities in Iowa is 384. I added a column for growth rate to my cities data frame containing the communities in Iowa that fall between our population parameters. Growth rate is calculated by dividing the change in population by the time period the change occurred.\n\nGrowth Rate = N/t\n\nI also plotted the twenty-five lowest growth rates in Iowa.\n\n\n\n\n\nKeokuk really stands out on this plot. Earlier in the Housing project, there was discussion about the deteriorating qualities of Lee County, the county of which Keokuk resides. This anomaly will need further investigated.\nNext, I want to look at which communities are growing, stable, and shrinking. The growth rates column should aid in this analysis."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#wednesday",
    "href": "posts/Week_5/Week_5.html#wednesday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Wednesday",
    "text": "Wednesday\nBecause I will be doing a lot of data visualization in the coming weeks with my demographics analysis task, I decided it would be a good idea to refresh my data visualization skills. I scored a 116 on the Data Visualization in R assessment I took on DataCamp. I am pretty happy with that score.\nI feel that I have quality skills in terms of data visualization, but I am lacking when it comes to organizing the data for visualizations. I think completely the track for Data Visualization in R would be helpful.\n\n\n\n\n\nWhile I was on DataCamp on Wednesday, I also completed the Introduction to Deep Learning with Keras course so I would be ready to create an AI Model. I am in charge of making the model that identifies clear images of houses. We are using Google Collab to create our models, and all the data is being stored in Google Drive. The “clear images” are sorted into three categories: Obstructed, Partially Obstructed, and Not Obstructed.\n\n\n\n\n\nI was successful in creating the AI Model on Wednesday. Below is the accuracy of my model. There is still a long way to go with it. The Housing team has a lot more photos to gather to train our models on and make them as accurate as possible. Now that our AI Models are created, we can go back to web scraping and gathering images."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#thursday",
    "href": "posts/Week_5/Week_5.html#thursday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Thursday",
    "text": "Thursday\nITAG Conference\nWork on Data Visualization in R DataCamp track."
  },
  {
    "objectID": "posts/Week_5/Week_5.html#friday",
    "href": "posts/Week_5/Week_5.html#friday",
    "title": "Week Five of Data Science for the Public Good",
    "section": "Friday\\",
    "text": "Friday\\"
  },
  {
    "objectID": "posts/Week_Four/Week_4.html",
    "href": "posts/Week_Four/Week_4.html",
    "title": "Week Four of Data Science for the Public Good",
    "section": "",
    "text": "On Monday this week, we met with the Director of the Community and Economic Development department at Iowa State University Extension and Outreach, Erin Olson-Douglas. She is one of our stakeholders for the Housing Project, and we had a great meeting with her. The agenda of the meeting was to update her on where we are at with the project. As of Monday, we were gathering data and ideas for the AI Image Model.\nShe had a couple of questions:\n\nIf the image gathered from Google Street View is bad, can the model be told to look at a different site for a better image?\nHow will the model choose one of the photos?\n\nIs the image chosen on a rating scale (good, usable, not usable)?\nIs the image chosen based on a simple yes or no?\n\n\nI ended the meeting with a question myself:\n\nHow do we get the model to work off of websites, not already available, pre-loaded images?\n\nThis is something I had not thought about yet. So far, we have been making, training, and testing models on images that we gathered and put into a database ourselves. I am not sure how we would get an AI Model to gather its own images, or if that is something we even need it to do.\nWe also ended the meeting with some knowledge on how we are presenting our final project. We need to keep in mind how our project can be picked up and used by others. We also need to make sure we are documenting and explaining our process so the next years of DSPG interns can continue this housing project.\nFinally, Erin Olson-Douglas is going to be arranging a meeting with a county assesor for us. We are very curious how assessors complete their jobs with houses. We want to know what it is that they look at and look for when completing the assessment. Erin thinks we will be meeting with the Polk County assessor.\n\n\n\nOn Monday, I also helped my team assemble Excel spreadsheets with all the addresses for Slater, Grundy Center, Independence, and New Hampton. I was in charge of the Slater data and part of the Grundy Center and Independence data sets.\nWe needed to first gather the addresses for each city. Gavin and Angelina used a spatial tool on Vanguard and Beacon to do this. On the map for the websites, the second tool from the left is called the Selection Tool. When you drag it over a section of properties, the list of parcels shows up in a “Results” section on the right.\nGavin used a web scraper attached to a Chrome Extension to then scrape the parcel information listed in “Results” from Beacon. Angelina was lucky and could download the parcel information as a .csv file from Vanguard.\n\n\n\n\n\nOnce the parcel information was scraped for Beacon, we had to go in and clean the data. Below is a sample of what the .csv file looked like after Gavin scraped it. The first thing we had to do was get the data all on one line. We used the following function in Excel to transform the street addresses onto one line:\n\n=TRIM(CLEAN(SUBSTITUTE(cell, CHAR(160), ” “)))\n\n\n\n\n\n\nWe then used the the “Text to Columns” tool in Excel to separate the data into Parcel ID, Street Address, and Owner. We used the “-” as a delimiter. We cleaned the data to remove any addresses that were obviously non-residential, and we also narrowed the data down to just the Parcel ID and address.\n\n\n\n\n\nFrom there, we created the URLs to gather Google Street View images.\nWe used the following function in Excel to transform the street addresses into workable addresses for Google Street View:\n\n=SUBSTITUTE(TRIM(cell),” “,”+“))\n\nThis function removes all spaces and replaces them with + signs.\nHere is the output of the cleaned Slater data:"
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#monday",
    "href": "posts/Week_Four/Week_4.html#monday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "",
    "text": "On Monday this week, we met with the Director of the Community and Economic Development department at Iowa State University Extension and Outreach, Erin Olson-Douglas. She is one of our stakeholders for the Housing Project, and we had a great meeting with her. The agenda of the meeting was to update her on where we are at with the project. As of Monday, we were gathering data and ideas for the AI Image Model.\nShe had a couple of questions:\n\nIf the image gathered from Google Street View is bad, can the model be told to look at a different site for a better image?\nHow will the model choose one of the photos?\n\nIs the image chosen on a rating scale (good, usable, not usable)?\nIs the image chosen based on a simple yes or no?\n\n\nI ended the meeting with a question myself:\n\nHow do we get the model to work off of websites, not already available, pre-loaded images?\n\nThis is something I had not thought about yet. So far, we have been making, training, and testing models on images that we gathered and put into a database ourselves. I am not sure how we would get an AI Model to gather its own images, or if that is something we even need it to do.\nWe also ended the meeting with some knowledge on how we are presenting our final project. We need to keep in mind how our project can be picked up and used by others. We also need to make sure we are documenting and explaining our process so the next years of DSPG interns can continue this housing project.\nFinally, Erin Olson-Douglas is going to be arranging a meeting with a county assesor for us. We are very curious how assessors complete their jobs with houses. We want to know what it is that they look at and look for when completing the assessment. Erin thinks we will be meeting with the Polk County assessor.\n\n\n\nOn Monday, I also helped my team assemble Excel spreadsheets with all the addresses for Slater, Grundy Center, Independence, and New Hampton. I was in charge of the Slater data and part of the Grundy Center and Independence data sets.\nWe needed to first gather the addresses for each city. Gavin and Angelina used a spatial tool on Vanguard and Beacon to do this. On the map for the websites, the second tool from the left is called the Selection Tool. When you drag it over a section of properties, the list of parcels shows up in a “Results” section on the right.\nGavin used a web scraper attached to a Chrome Extension to then scrape the parcel information listed in “Results” from Beacon. Angelina was lucky and could download the parcel information as a .csv file from Vanguard.\n\n\n\n\n\nOnce the parcel information was scraped for Beacon, we had to go in and clean the data. Below is a sample of what the .csv file looked like after Gavin scraped it. The first thing we had to do was get the data all on one line. We used the following function in Excel to transform the street addresses onto one line:\n\n=TRIM(CLEAN(SUBSTITUTE(cell, CHAR(160), ” “)))\n\n\n\n\n\n\nWe then used the the “Text to Columns” tool in Excel to separate the data into Parcel ID, Street Address, and Owner. We used the “-” as a delimiter. We cleaned the data to remove any addresses that were obviously non-residential, and we also narrowed the data down to just the Parcel ID and address.\n\n\n\n\n\nFrom there, we created the URLs to gather Google Street View images.\nWe used the following function in Excel to transform the street addresses into workable addresses for Google Street View:\n\n=SUBSTITUTE(TRIM(cell),” “,”+“))\n\nThis function removes all spaces and replaces them with + signs.\nHere is the output of the cleaned Slater data:"
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#tuesday",
    "href": "posts/Week_Four/Week_4.html#tuesday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Tuesday",
    "text": "Tuesday\nOn Tuesday morning, the entire DSPG group went to Slater to get some practice for the WINVEST project. We walked around with a city council member so we would have some local knowledge of the town while we were practicing. The local knowledge was very helpful, as we would not have known what some of the downtown commercial buildings were used for without the city council member.\nWe discovered on Tuesday that gutters are not the best test of our AI Model because most houses in Slater either had perfectly fine gutters or had no gutters at all. We do not think we will be able to get enough images to successfully train an AI Model to identify damaged gutters.\nTuesday afternoon was spent web scraping. I did the DataCamp training for web scraping in R about a week and a half ago, which was understandable then. It could have been better when applying it in real-world scenarios. Angelina and I have done a lot of Googleing to find other examples to help us. We were tasked earlier with scraping county housing assessor data. Independence in Buchanan County is on Vanguard. Slater in Story County, Grundy Center in Grundy County, and New Hampton in Chickasaw County are all on Beacon. We needed help figuring out how to scrape from these sites this week.\nI successfully scraped the categories of shoes from my favorite shoe site, Jonak, though.\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.1.3\n\njonak = \"https://www.jonak-paris.com/collection/shoes/sandals.html\"\ncategories &lt;- read_html(jonak) %&gt;% html_elements(\".categ_itm_name\") %&gt;% html_text2()\nhead(categories)\n\n[1] \"New in\"              \"Mules, Clogs\"        \"Sandals\"            \n[4] \"Beach sandals\"       \"Wedges, Espadrilles\" \"Babies, salomes\"    \n\n\nI wanted to know if Beacon and Vanguard had anti-web scraping protections on them, and that’s why Angelina and I were unsuccessful in scraping them. I found a function online called paths_allowed() in the robotstxt package that checks to see if there are protections. Both Beacon and Vanguard have protections from running the URLs through the function. Jonak doesn’t, so it was easy to scrape from the site. Zillow doesn’t have any protections, either.\n\nlibrary(robotstxt)\n\nWarning: package 'robotstxt' was built under R version 4.1.3\n\n#TRUE = web scraping allowed, FALSE = web scraping not allowed\npaths_allowed(\"https://beacon.schneidercorp.com/Application.aspx?AppID=165&LayerID=2145&PageTypeID=3&PageID=1107&Q=1818183221\")\n\n\n beacon.schneidercorp.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://buchanan.iowaassessors.com/results.php?mode=basic&history=-1&ipin=%25&idba=&ideed=&icont=&ihnum=&iaddr=&ilegal=&iacre1=&iacre2=&iphoto=0\")\n\n\n buchanan.iowaassessors.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://www.zillow.com/homedetails/2925-Arbor-St-Ames-IA-50014/93961907_zpid/\")\n\n\n www.zillow.com                      \n\n\n[1] TRUE\n\npaths_allowed(\"https://www.jonak-paris.com/collection/shoes/sandals.html\")\n\n\n www.jonak-paris.com                      \n\n\n[1] TRUE\n\n\nBecause Zillow doesn’t have protections, the housing team decided to switch tactics. The Housing Team had decided earlier to scrape Zillow and Trulia alongside Vanguard and Beacon for housing data. When we started the scraping of Trulia, we learned that Zillow owns Trulia. This was a huge win for us because that meant we only had to scrape one of the sites. We chose Zillow because it provides Zestimates, estimates of housing price based on external factors, and Trulia does not."
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#wednesday-and-thursday",
    "href": "posts/Week_Four/Week_4.html#wednesday-and-thursday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Wednesday and Thursday",
    "text": "Wednesday and Thursday\nOn Wednesday, I was tasked with scraping the houses in Slater, IA. I made this data frame in R with the data I scraped from Zillow.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter()         masks stats::filter()\nx readr::guess_encoding() masks rvest::guess_encoding()\nx dplyr::lag()            masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#### Pulling recently SOLD houses ######\n########################################\n\nsold = \"https://www.zillow.com/slater-ia/sold/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%2C%22rs%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the url\nss = read_html(sold)\n\n# lists how many records there are to pull from\nhousesold &lt;- read_html(sold) %&gt;% html_elements(\"article\")\n\n#create a dataframe with addresses, prices, bathrooms, bedrooms, and square footage of all SOLD houses\nres_ss &lt;- tibble(\n      address= ss %&gt;% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;% html_text(),\n      price = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %&gt;% html_text(),\n      bedrooms = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %&gt;% \n        html_text(),\n      bathrooms = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %&gt;% \n        html_text(),\n      sqft = ss %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %&gt;% \n        html_text()\n    ) \n\n    \n##### Pulling FOR SALE houses #####\n######################################\n\nsale = \"https://www.zillow.com/slater-ia/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the webpage\npg = read_html(sale)\n\n# get list of houses for sale that appears on the page\n# each property card is called an article when you inspect the webpage\nhousesale &lt;- read_html(sale)%&gt;%html_elements(\"article\")\n\n# create a dataframe for the FOR SALE houses\nres_pg &lt;- tibble(\n  address= pg %&gt;% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div[1]/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;% html_text(),\n  price = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %&gt;% html_text(),\n  bedrooms = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %&gt;% \n    html_text(),\n  bathrooms = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %&gt;% \n    html_text(),\n  sqft = pg %&gt;% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %&gt;% \n    html_text()\n) \n\n# combine recently SOLD and FOR SALE houses in one data frame\nresults &lt;- res_pg %&gt;% bind_rows(res_ss)\nprint(results)\n\n# A tibble: 12 x 5\n   address                             price    bedrooms bathrooms sqft \n   &lt;chr&gt;                               &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;\n 1 201 10th Ave, Slater, IA 50244      $295,000 3        2         924  \n 2 50287 210th Hwy, Slater, IA 50244   $475,000 4        2         2,147\n 3 1013 Redbud Dr, Slater, IA 50244    $429,900 4        3         1,884\n 4 506 8th Ave, Slater, IA 50244       $328,000 4        3         1,180\n 5 604 Story St, Slater, IA 50244      $200,000 3        1         1,359\n 6 611 1st Ave N, Slater, IA 50244     $265,000 4        2         1,116\n 7 101 Main St, Slater, IA 50244       $255,000 5        3         1,896\n 8 1015 Redbud Dr, Slater, IA 50244    $397,732 2        3         1,325\n 9 107 Main St, Slater, IA 50244       $242,500 4        2         1,515\n10 52898 Highway 210, Slater, IA 50244 $400,000 4        1.75      2,550\n11 604 Marshall St, Slater, IA 50244   $135,000 3        1         1,166\n12 104 N Benton St, Slater, IA 50244   $210,000 3        2         1,056\n\n\nThis web scraping was really hard. I spent a lot of time understanding the xpaths and why I was using them. I think this was beneficial to me though. I got the xpaths for my code by inspecting the Zillow webpage. To inspect, you right click on the web page and then select “Inspect.” This will open up a screen that shows you the HTML in the web page.\nIf you right click on any element in the HTML you can select “Copy” and then “Full xpath” to copy the xpath of an element. There were some minor changes that needed to be made to the xpaths. Below is an example:\n\n/html/body/div[1]/div[5]/div/div/div/div/div[1]/ul/li[1]/div/div/article/div/div[1]/a/address\nv.\n/html/body/div[1]/div[5]/div/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/a/address\n\nThe difference in these two xpaths is what comes after the li element. The first xpath selects only the first li in the HTML. The second xpath selects all li elements in the HTML. The second version allows you to get all of the children of all li elements as well."
  },
  {
    "objectID": "posts/Week_Four/Week_4.html#friday",
    "href": "posts/Week_Four/Week_4.html#friday",
    "title": "Week Four of Data Science for the Public Good",
    "section": "Friday",
    "text": "Friday\nI spent a lot of time on Thursday creating the Team blog for this week. We gave a presentation to an outside individual from Oklahoma State University Friday morning, so I made sure to put a lot of my attention towards that.\nAfter the presentation on Friday morning, I got back to web scraping. Gavin was able to scrape some images from Zillow on Thursday night, and he shared the code he used with Angelina and me. The code is below:\n\n# webpage to scrape. This specific link brings you to the grundy center houses for sale.\nzillow_url_grundy &lt;- \"https://www.zillow.com/grundy-center-ia/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22usersSearchTerm%22%3A%22Grundy%20Center%2C%20IA%22%2C%22mapBounds%22%3A%7B%22west%22%3A-93.21166512207031%2C%22east%22%3A-92.40828987792969%2C%22south%22%3A42.153050722920995%2C%22north%22%3A42.55594363773797%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A24980%2C%22regionType%22%3A6%7D%5D%2C%22isMapVisible%22%3Afalse%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A11%7D\"\nwebpage_grundy &lt;- read_html(zillow_url_grundy)\n\n# gathers addresses. This xpath can be obtained by right clicking on the address you want and clicking inspect.\n# you then must navigate to the html section that contains the text. right click again and go to copy -&gt; full xpath\n# to gather all addresses on page the full xpath must be altered for example this xpath below has li// which signifies select all children where the original xpath would just have li/...\naddresses &lt;- webpage_grundy %&gt;%\n  html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div[1]/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %&gt;%\n  html_text()\nprint(addresses)\n\n\n# gathers image links. Similair method as above\nimage_urls &lt;- webpage_grundy %&gt;%\n  html_nodes(xpath = '//*[@id=\"swipeable\"]/div[1]/a/div/img') %&gt;%\n  html_attr(\"src\")\n\nprint(image_urls)\n\n#downloads first item\n#download.file(image_urls[1], \"image.png\", mode = \"wb\")\n\n# creates folder for images scraped then iterativly names each image (1-9 in this case).\n# More specifically it takes the image links gathered above, goes to each link, and downloads the image\n# dir.create makes a new folder. You only need to run this once.Everytime you do file.path to that folder it will add newly downloaded images to that folder\n# This method simply names each image 1 - number of images\ndir.create(\"images_grundy_sale\")\nfor (i in seq_along(image_urls)) {\n  file_path &lt;- file.path(\"images_grundy_sale\", paste0(\"image_\", i, \".png\"))\n  download.file(image_urls[i], file_path, mode = \"wb\")\n  print(file_path)\n}\n\n#creates folder for images scraped then names them based on address they were scraped with\n# same as above except for how the images are named. for each image the address grabbed earlier is printed as the name.\n# this returns (image_ 123 main st) for example\n# you can alter this to return our naming convention (source_city_address_) by replacing \"image_\" with the source and city\n# for example if you are pulling slater images from zillow it will be paste0(\"Z_S_\", address, \"_.png\") which will print the titles of images as Z_S_123 main st_.png\n# Z (Zillow) G (Google) V (Vanguard) B (Beacon) :: S (Slater) H (New Hampton) D (Independence) G (Grundy Center)\ndir.create(\"images_grundy_sale_addresses\")\nfor (j in seq_along(image_urls)) {\n  address &lt;- addresses[j]\n  file_name &lt;- paste0(\"image_\", address, \".png\")\n  file_path &lt;- file.path(\"images_grundy_sale_addresses\", file_name)\n  download.file(image_urls[j], file_path, mode = \"wb\")\n  print(file_path)\n}\n\nNext week, I will get around the anti-web scraping protections Beacon and Vanguard have on their sites. Beacon and Vanguard have information on houses that aren’t listed on Zillow, and more pictures aren’t listed on Zillow or Google Street View.\n\nNotes to self:\ngit pull\ngit add .\ngit commit -m “message”\ngit push"
  }
]