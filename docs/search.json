[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kailyn Hogan’s DSPG Blog",
    "section": "",
    "text": "Team Blog: Week Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Four\n\n\n\n\n\n\n\nWeek Four\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek Three\n\n\n\n\n\n\n\nWeek Three\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\nWeek Two\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\n  \n\n\n\n\nWeek One\n\n\n\n\n\n\n\nWeek One\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nKailyn Hogan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Week_1/Week_1.html",
    "href": "posts/Week_1/Week_1.html",
    "title": "Week One",
    "section": "",
    "text": "The priority for week one of Data Science for the Public Good was DataCamp training. I completed the following list of DataCamp courses this week.\n\n\n\nAI Fundamentals\nGitHub Concepts\nR Programming Assessment\nUnderstanding and Interpreting Data Assessment\nIntroduction to R\nIntroduction to the Tidyverse"
  },
  {
    "objectID": "posts/Week_2/Week_2.html",
    "href": "posts/Week_2/Week_2.html",
    "title": "Week Two",
    "section": "",
    "text": "This week we were introduced to the TidyCensus package via the 2023 webinar series Analyzing 2017-2021 ACS Data in R and Python by Kyle Walker, Associate Professor at Texas Christian University and R developer.\nWe watched the first two videos in the webinar series: Working with the 2021 American Community Survey with R and Tidycensus and Mapping and spatial analysis with ACS data in R.\nI created the following plots using the information I learned from the webinars and previous knowledge:\n\nlibrary(tidycensus)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(ggthemes)\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.1.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\n\n##ONWER VS. RENTER OCCUPIED FOR EACH CITY\nown_iowa <- get_decennial(geography = \"place\",\n                          state = \"IA\",\n                          year = 2010,\n                          output = \"wide\",\n                          variable = c(\"H017003\",\"H017004\",\"H017005\",\"H017006\",\"H017007\",\"H017008\",\"H017009\",\"H017010\",\"H017011\")) %>% \n  mutate(tenure = \"Owner\") %>% \n  rename(fifteentotwentyfour = H017003, twentyfivetothirtyfour = H017004, thirtyfivetofourtyfour = H017005, fourtyfivetofiftyfour = H017006, fiftyfivetofiftynine = H017007, sixtytosixtyfour = H017008, sixtyfivetoseventyfour = H017009, seventyfivetoeightyfour = H017010, overeightyfive = H017011)\n\nGetting data from the 2010 decennial Census\n\n\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\nrent_iowa <- get_decennial(geography = \"place\",\n                           state = \"IA\",\n                           year = 2010,\n                           output = \"wide\",\n                           variable = c(\"H017013\",\"H017014\", \"H017015\", \"H017016\", \"H017017\",\"H017018\",\"H017019\", \"H017020\",\"H017021\")) %>% \n  mutate(tenure = \"Renter\") %>% \n  rename(fifteentotwentyfour = H017013, twentyfivetothirtyfour = H017014, thirtyfivetofourtyfour = H017015, fourtyfivetofiftyfour = H017016, fiftyfivetofiftynine = H017017, sixtytosixtyfour = H017018, sixtyfivetoseventyfour = H017019, seventyfivetoeightyfour = H017020, overeightyfive = H017021)\n\nGetting data from the 2010 decennial Census\nUsing Census Summary File 1\n\n## Getting data from the 2010 decennial Census\n## Using Census Summary File 1\niowa <- rent_iowa %>% \n  bind_rows(own_iowa)%>% \n  pivot_longer(-c(NAME, GEOID, tenure),\n               names_to = \"agegroups\",\n               values_to = \"count\")\n\n###plots for grundy, independence and new hampton for age break downs by housing tenure\niowa %>% \n  filter(NAME == \"New Hampton city, Iowa\") %>% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %>% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +10, -count - 12), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in New Hampton, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %>% \n  filter(NAME == \"Grundy Center city, Iowa\") %>% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %>% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +5, -count - 8), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Grundy Center, IA \\nby Age and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\niowa %>% \n  filter(NAME == \"Independence city, Iowa\") %>% \n  mutate(agegroups = fct_relevel(agegroups, c(\"fifteentotwentyfour\", \"twentyfivetothirtyfour\",\"thirtyfivetofourtyfour\",\"fourtyfivetofiftyfour\",\"fiftyfivetofiftynine\",\"sixtytosixtyfour\",\"sixtyfivetoseventyfour\",\"seventyfivetoeightyfour\",\"overeightyfive\"))) %>% \n  ggplot(aes(x = agegroups, y = if_else(tenure == \"Renter\", count, -count))) +\n  geom_bar(aes(fill = tenure), stat = \"identity\") +\n  geom_text(aes(x = agegroups, y = if_else(tenure == \"Renter\", count +12, -count - 15), label = scales::comma(count))) +\n  coord_flip()+\n  scale_x_discrete(labels = c(\"15 to 24\", \"25 to 34\", \"35 to 44\", \"45 to 54\", \"55 to 59\", \"60 to 64\", \"65 to 74\", \"75 to 84\", \"85 and Older\")) +\n  scale_y_continuous(label = abs)+\n  labs(x = \"\",\n       y = \"Population\",\n       fill = \"\",\n       title = \"Population in Independence, IA by \\nAge and Tenure\",\n       subtitle = \"2010 Decennial Census\") +\n  theme_fivethirtyeight()+\n  theme(legend.position = \"bottom\") +\n  scale_fill_wsj()\n\n\n\n\n\n##MEDIAN AGE\n\nmedage <- c(\"medage\" = \"B01002_001\")\n\ngrundy <- get_acs(geography = \"place\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %>% \n  filter(NAME == \"Grundy Center city, Iowa\") %>% \n  mutate(year = 2021) \n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\ninde <- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %>% \n  filter(NAME == \"Independence city, Iowa\") %>% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\nnew <- get_acs(geography = \"place\",\n                state = \"IA\",\n                variable = medage,\n                year = 2021,\n                output = \"tidy\") %>% \n  filter(NAME == \"New Hampton city, Iowa\") %>% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nia <- get_acs(geography = \"state\",\n              state = \"IA\",\n              variable = medage,\n              year = 2021,\n              output = \"tidy\") %>% \n  mutate(year = 2021)\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmedage16_20 <- grundy %>% \n  bind_rows(ia,inde, new) %>% \n  mutate(upper = estimate + moe,\n         lower = estimate - moe)\n\nmedage16_20 %>% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = estimate, ymin = lower, ymax = upper))+\n  geom_line(aes(x = NAME, y = estimate))+\n  coord_flip()+\n  geom_text(aes(x = NAME, y = estimate, label = estimate), hjust = .5, vjust = -.8)+\n  scale_x_discrete(limits = c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\", \"Iowa\"),labels = c(\"Grundy Center\",\"Independence\",\"New Hampton\",\"Iowa\"))+\n  labs(title = \"Median Age of the Population\",\n       subtitle = \"Data aquired from 2017-2021 5-year ACS estimates.\",\n       x = \"\",\n       y = \" \",)+\n  theme_fivethirtyeight()\n\n`geom_line()`: Each group consists of only one observation.\ni Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n#PERCENT FOREIGN BORN, NON-CITIZENS\nforeign <- c(\"foreign\" = \"B05012_003\",\n             \"pop\" = \"B05012_001\")\n\nforeign <- get_acs(geography = \"place\",\n                   state = \"IA\",\n                   year = 2021,\n                   variable = foreign,\n                   output = \"wide\") %>% \n  filter(NAME %in% c(\"Grundy Center city, Iowa\", \"Independence city, Iowa\", \"New Hampton city, Iowa\"))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2017-2021 5-year ACS\nforeign <- foreign %>% \n  mutate(pct_foreign = foreignE/popE,\n         pct_foreign_moe = moe_prop(foreignE, popE, foreignM, popM))\n\nforeign %>% \n  ggplot() +\n  geom_pointrange(aes(x = NAME, y = pct_foreign, ymin = pct_foreign - pct_foreign_moe, ymax = pct_foreign + pct_foreign_moe ))+\n  coord_flip() +\n  scale_y_continuous(label = scales::percent) +\n  theme_fivethirtyeight() +\n  labs( x = \" \",\n        y = \"Pct Foreign\",\n        title = \"Percent of Foreign-Born, Non-Citzen\",\n        subtitle = \"2017-2021 5-Year ACS Estimates\")+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  geom_text(aes(x = NAME, y = pct_foreign, label = percent(pct_foreign)), hjust = .5, vjust = -.8)\n\n\n\n\n\n##MEDIAN INCOME BY HOUSEHOLD\ngrundy_acs <- get_acs(state = \"IA\", \n                       geography = \"place\",\n                       year = 2021,\n                       variable = c(med_house = \"B19013_001\"),\n                       output = \"tidy\") %>% \n  filter(NAME == \"Grundy Center city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_acs <- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %>% \n  filter(NAME == \"Independence city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_acs <- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(med_house = \"B19013_001\"),\n                      output = \"tidy\") %>% \n  filter(NAME == \"New Hampton city, Iowa\")\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\nmed_house <- grundy_acs %>% \n  bind_rows(inde_acs,new_acs)\nmed_house %>%  \n  ggplot(aes(x = NAME, y = estimate))+\n  geom_pointrange(aes(ymin = estimate - moe, ymax = estimate +moe))+\n  geom_text(aes(label = scales::dollar(estimate)), hjust = -.2)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  scale_y_continuous(label = scales::dollar)+\n  labs(y = \"\",\n       title = \"Estimated Median Income by Household\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\n\n# % LABOR FORCE UNEMPLOYED\ngrundy_un <- get_acs(state = \"IA\", \n                      geography = \"place\",\n                      year = 2021,\n                      variable = c(\"total\" = \"B23025_003\",\n                                   \"unemployed\" = \"B23025_005\"),\n                      output = \"wide\") %>% \n  filter(NAME == \"Grundy Center city, Iowa\") %>% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\ninde_un <- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %>% \n  filter(NAME == \"Independence city, Iowa\") %>% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\nnew_un <- get_acs(state = \"IA\", \n                     geography = \"place\",\n                     year = 2021,\n                     variable = c(\"total\" = \"B23025_003\",\n                                  \"unemployed\" = \"B23025_005\"),\n                     output = \"wide\") %>% \n  filter(NAME == \"New Hampton city, Iowa\") %>% \n  mutate(pct = unemployedE / totalE,\n         moe = moe_ratio(unemployedE, totalE, unemployedM, totalM))\n\nGetting data from the 2017-2021 5-year ACS\n\n## Getting data from the 2016-2020 5-year ACS\npct_un <- grundy_un %>% \n  bind_rows(inde_un,new_un)\npct_un %>%  \n  ggplot(aes(x = NAME, y = pct))+\n  geom_pointrange(aes(ymin = pct - moe, ymax = pct +moe))+\n  geom_text(aes(label = scales::percent(pct)), hjust = -.2)+\n  scale_y_continuous(label = scales::percent)+\n  scale_x_discrete(labels = c(\"Grundy Center\", \"Independence\", \"New Hampton\"))+\n  labs(y = \"\",\n       x = \"\",\n       title = \"Estimated % of Population Unemployed\",\n       subtitle = \"Data acquired from 2017-2021 5-year ACS estimates.\")+\n  theme_fivethirtyeight()\n\n\n\n\nWe also had our first client meeting for the Housing and AI project this week on Thursday, May 25th. We gained clarity for which direction we should be heading in the project from our stakeholders.\ngit_add\ngit_commit\ngit_push"
  },
  {
    "objectID": "posts/Week_3/Week_Three.html",
    "href": "posts/Week_3/Week_Three.html",
    "title": "Week Three",
    "section": "",
    "text": "This week my team has been trying to gather resources to use in our AI model. We are trying to scrape data from multiple different sources including Beacon, Vanguard, and Trulia to compile photos and train our AI model.\nI have been learning how to web scrape, and I have completed the following DataCamp trainings:\n\n\n\nIntermediate R\nWeb Scraping in R\n\nI have also created an R Markdown file to document my web scraping practice.\nI did a quick Google search for a web scraper for Beacon before I attempted it myself, and I found a GitHub page dedicated to one:\nhttps://github.com/openaddresses/machine/issues/580\nI am not sure it is relevant to the data I am trying to scrape from Beacon.\nI also learned how to create a quarto blog this week!\n\n\n\nTo better understand the AI model my group is trying to create, I am watching the following YouTube videos:\n\nhttps://www.youtube.com/watch?v=19LQRx78QVU&list=PLgNJO2hghbmiXg5d4X8DURJP9yv9pgjIu&index=1&ab_channel=NicholasRenotte\nhttps://www.youtube.com/watch?v=jztwpsIzEGc&ab_channel=NicholasRenotte\n\n\n\n\n\n\n\nShowing the rating scale for the DSM housing project.\n\n\nI also gave the Morning Coffee Talk on Thursday this week over the Des Moines Housing Project I was a part of. The Des Moines Housing Project was conducted by czb, a firm located in Bath, Maine. I was hired as a student researcher for them this past spring, and I conducted housing surveys on roughly 6,000 properties in Southwestern Des Moines."
  },
  {
    "objectID": "posts/Week_4/Team Blog.html#project-overview",
    "href": "posts/Week_4/Team Blog.html#project-overview",
    "title": "Team Blog: Week Four",
    "section": "Project Overview",
    "text": "Project Overview\nThis is the project plan we came up with the first week of DSPG. This project is intended to span over three years with DPSG, and different interns will be working on it in the coming years. Thus, the project plan is ambitious for this summer."
  },
  {
    "objectID": "posts/Week_4/Team Blog.html#problem-statement",
    "href": "posts/Week_4/Team Blog.html#problem-statement",
    "title": "Team Blog: Week Four",
    "section": "Problem Statement",
    "text": "Problem Statement\nThe absence of a comprehensive and unbiased assessment of housing quality in rural communities poses challenges in identifying financing gaps and effectively allocating resources for housing improvement. Consequently, this hinders the overall well-being and health of residents, impacts workforce stability, diminishes rural vitality, and undermines the economic growth of Iowa. Moreover, the subjective nature of evaluating existing housing conditions and the limited availability of resources for thorough investigations further compound the problem. To address these challenges, there is a pressing need for an AI-driven approach that can provide a more accurate and objective evaluation of housing quality, identify financing gaps, and optimize the allocation of local, state, and federal funds to maximize community benefits.\nUtilizing web scraping techniques to collect images of houses from various assessor websites, an AI model can be developed to analyze and categorize housing features into good or poor quality. This can enable targeted investment strategies. It allows for the identification of houses in need of improvement and determines the areas where financial resources should be directed. By leveraging AI technology in this manner, the project seeks to streamline the housing evaluation process, eliminate subjective biases, and facilitate informed decision-making for housing investment and development initiatives in rural communities"
  },
  {
    "objectID": "posts/Week_4/Team Blog.html#goals-and-objectives",
    "href": "posts/Week_4/Team Blog.html#goals-and-objectives",
    "title": "Team Blog: Week Four",
    "section": "Goals and Objectives",
    "text": "Goals and Objectives\n\nGenerate Google Street View urls for Slater, Independence, Grundy Center, and New Hampton\nScrape available housing data for Slater, Independence, Grundy Center, and New Hampton\n\nZillow\nRealtors.com\nBeacon\nVanguard\n\nCombine data frames\nCreate AI models"
  },
  {
    "objectID": "posts/Week_4/Team Blog.html#our-progress",
    "href": "posts/Week_4/Team Blog.html#our-progress",
    "title": "Team Blog: Week Four",
    "section": "Our Progress",
    "text": "Our Progress\nWe have been making good progress to complete the goals and objectives we outlined above. Since the beginning of the Data Science for the Public Good Program, we have been expanding our knowledge of data science, particularly in areas that relate to this housing project. We have been learning and covering new concepts through DataCamp. We have also watched two webinars on TidyCensus training, as well as started creating AI Models to practice with.\n\nData Camp Training:\n\nGitHub Concepts\nAI Fundamentals\nIntroduction to R\nIntermediate R\nIntroduction to the Tidyverse\nWeb Scraping in R\nIntroduction to Deep Learning with Keras\n\n\n\nTidyCensus Demographic Data Collection:\nOne of the first steps in our project was to explore the available demographic data in our selected cities and counties. We thought it valuable to understand the demographic data, and we have represented in the plots below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Test AI Models:\nThe next step was creating an AI Model. We decided to create an AI Model early in the project before finishing the housing data collection so that we had a better understanding when it came to putting everything together. The AI Model below tests for vegetation in front of houses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Week:\n\nIn-Person Data Collection\nOn Tuesday this week, the entire DSPG program went to Slater to practice data collection in person. The housing group took this as an opportunity to collect some housing photos on the ground to use in our AI Model later on.\nDo we have photos to use here?\n\n\nGoogle Street View and URLs\nWe are getting the majority of our photos for the AI to use from Google Street View. Google has an API key that you can use to generate an image for a specific address. We spent the first half of this week pulling addresses from each of our cities and creating URLs to pull the images from Google Street View.\nWe ran into a couple of problems when doing this, the biggest of which is displayed in the images below. Because we are working with cities in rural areas, there is not Google Street View images available for every street in our cities.\n\n\n\nGoogle Street View information for Grundy Center, Iowa. For reference, population was 2,811 as of 2023.\n\n\n\n\n\nGoogle Street View information for Slater, Iowa. For reference, population was 1,639 as of 2023.\n\n\n\n\n\nGoogle Street View information for Independence, Iowa. For reference, population was 6,307 as of 2023.\n\n\n\n\n\nGoogle Street View information for New Hampton, Iowa. For reference, population was 3,368 as of 2023.\n\n\nBelow is a sample from the tables we created containing the URLs to grab the images from Google Street View.\n\n\n\n\n\n\n\n\n\n\n\n\nParcel ID\nStreet\nCity State\nFull Address\nFront URL\nBack URL\nFull URL\n\n\n\n\n1331228310\n701 BENTON CIR\nSLATER IOWA\n701 BENTON CIR, SLATER IOWA\n701+BENTON+CIR,+SLATER+IOWA\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=701+BENTON+CIR,+SLATER+IOWA\n\n\n1331210210\n637 GREENE ST\nSLATER IOWA\n637 GREENE ST, SLATER IOWA\n637+GREENE+ST,+SLATER+IOWA\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=637+GREENE+ST,+SLATER+IOWA\n\n\n1331228210\n705 TAMA CIR\nSLATER IOWA\n705 TAMA CIR, SLATER IOWA\n705+TAMA+CIR,+SLATER+IOWA\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=705+TAMA+CIR,+SLATER+IOWA\n\n\n1331220100\n608 8TH AVE\nSLATER IOWA\n608 8TH AVE, SLATER IOWA\n608+8TH+AVE,+SLATER+IOWA\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=608+8TH+AVE,+SLATER+IOWA\n\n\n1331220080\n604 8TH AVE\nSLATER IOWA\n604 8TH AVE, SLATER IOWA\n604+8TH+AVE,+SLATER+IOWA\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=\nhttps://maps.googleapis.com/maps/api/streetview?size=800x800&location=604+8TH+AVE,+SLATER+IOWA\n\n\n\n\n\nWeb Scraping\nOnce we were finished collecting addresses and generating URLs, we moved on to scraping the web for more images. We decided to grab images from Zillow, Realtors.com, and the county assessor pages for our cities. We were able to successfully scrape images from Zillow this week.\ninsert images of web scraping progress after today"
  },
  {
    "objectID": "posts/Week_4/Team Blog.html#future-plans-and-next-steps",
    "href": "posts/Week_4/Team Blog.html#future-plans-and-next-steps",
    "title": "Team Blog: Week Four",
    "section": "Future Plans and Next Steps",
    "text": "Future Plans and Next Steps\nOnce we are able to scrape enough images off of Zillow, Realtors.com, and the assessor pages, we will be able to move on with creating AI Models. The diagram below outlines how the AI Models will work in the next steps of out project."
  },
  {
    "objectID": "posts/Week_4/Week_4.html",
    "href": "posts/Week_4/Week_4.html",
    "title": "Week Four",
    "section": "",
    "text": "Week Four of DSPG\nThe main focus for week four on the housing team was web scraping. I did the DataCamp training for web scraping in R about a week and a half ago, which was understandable then. It could have been better when applying it in real-world scenarios. Angelina and I have done a lot of Googleing to find other examples to help us. We were tasked with scraping county housing assessor data. Independence in Buchanan County is on Vanguard. Slater in Story County, Grundy Center in Grundy County, and New Hampton in Chickasaw County are all on Beacon. We needed help figuring out how to scrape from these sites this week.\nI successfully scrape the categories of shoes from my favorite shoe site, Jonak, though.\n\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.1.3\n\njonak = \"https://www.jonak-paris.com/collection/shoes/sandals.html\"\ncategories <- read_html(jonak) %>% html_elements(\".categ_itm_name\") %>% html_text2()\nhead(categories)\n\n[1] \"New in\"              \"Mules, Clogs\"        \"Sandals\"            \n[4] \"Beach sandals\"       \"Wedges, Espadrilles\" \"Babies, salomes\"    \n\n\nI wanted to know if Beacon and Vanguard had anti-web scraping protections on them, and that’s why Angelina and I were unsuccessful in scraping them. I found a function online called paths_allowed() in the robotstxt package that checks to see if there are protections. Both Beacon and Vanguard have protections from running the URLs through the function. Jonak doesn’t, so it was easy to scrape from the site. Zillow doesn’t have any protections, either.\n\nlibrary(robotstxt)\n\nWarning: package 'robotstxt' was built under R version 4.1.3\n\n#TRUE = web scraping allowed, FALSE = web scraping not allowed\npaths_allowed(\"https://beacon.schneidercorp.com/Application.aspx?AppID=165&LayerID=2145&PageTypeID=3&PageID=1107&Q=1818183221\")\n\n\n beacon.schneidercorp.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://buchanan.iowaassessors.com/results.php?mode=basic&history=-1&ipin=%25&idba=&ideed=&icont=&ihnum=&iaddr=&ilegal=&iacre1=&iacre2=&iphoto=0\")\n\n\n buchanan.iowaassessors.com                      \n\n\n[1] FALSE\n\npaths_allowed(\"https://www.zillow.com/homedetails/2925-Arbor-St-Ames-IA-50014/93961907_zpid/\")\n\n\n www.zillow.com                      \n\n\n[1] TRUE\n\npaths_allowed(\"https://www.jonak-paris.com/collection/shoes/sandals.html\")\n\n\n www.jonak-paris.com                      \n\n\n[1] TRUE\n\n\nBecause Zillow doesn’t have protections, the housing team decided to switch tactics and scrape from Zillow instead this week. I was tasked with scraping the houses in Slater, IA. I made this data frame in R with the data I scraped from Zillow.\n\nlibrary(rvest)\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.1.3\n\n\nWarning: package 'ggplot2' was built under R version 4.1.3\n\n\nWarning: package 'tibble' was built under R version 4.1.3\n\n\nWarning: package 'tidyr' was built under R version 4.1.3\n\n\nWarning: package 'readr' was built under R version 4.1.3\n\n\nWarning: package 'purrr' was built under R version 4.1.3\n\n\nWarning: package 'dplyr' was built under R version 4.1.3\n\n\nWarning: package 'stringr' was built under R version 4.1.3\n\n\nWarning: package 'forcats' was built under R version 4.1.3\n\n\nWarning: package 'lubridate' was built under R version 4.1.3\n\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.1     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter()         masks stats::filter()\nx readr::guess_encoding() masks rvest::guess_encoding()\nx dplyr::lag()            masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n#### Pulling recently SOLD houses ######\n########################################\n\nsold = \"https://www.zillow.com/slater-ia/sold/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%2C%22rs%22%3A%7B%22value%22%3Atrue%7D%2C%22fsba%22%3A%7B%22value%22%3Afalse%7D%2C%22fsbo%22%3A%7B%22value%22%3Afalse%7D%2C%22nc%22%3A%7B%22value%22%3Afalse%7D%2C%22cmsn%22%3A%7B%22value%22%3Afalse%7D%2C%22auc%22%3A%7B%22value%22%3Afalse%7D%2C%22fore%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the url\nss = read_html(sold)\n\n# lists how many records there are to pull from\nhousesold <- read_html(sold) %>% html_elements(\"article\")\n\n#create a dataframe with addresses, prices, bathrooms, bedrooms, and square footage of all SOLD houses\nres_ss <- tibble(\n      address= ss %>% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %>% html_text(),\n      price = ss %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %>% html_text(),\n      bedrooms = ss %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %>% \n        html_text(),\n      bathrooms = ss %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %>% \n        html_text(),\n      sqft = ss %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %>% \n        html_text()\n    ) \n\n    \n##### Pulling FOR SALE houses #####\n######################################\n\nsale = \"https://www.zillow.com/slater-ia/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A41.930365556704984%2C%22east%22%3A-93.55027834838869%2C%22south%22%3A41.782563414617314%2C%22west%22%3A-93.76760165161134%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%2C%22sche%22%3A%7B%22value%22%3Afalse%7D%2C%22schm%22%3A%7B%22value%22%3Afalse%7D%2C%22schh%22%3A%7B%22value%22%3Afalse%7D%2C%22schp%22%3A%7B%22value%22%3Afalse%7D%2C%22schr%22%3A%7B%22value%22%3Afalse%7D%2C%22schc%22%3A%7B%22value%22%3Afalse%7D%2C%22schu%22%3A%7B%22value%22%3Afalse%7D%2C%22land%22%3A%7B%22value%22%3Afalse%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A20522%2C%22regionType%22%3A6%7D%5D%2C%22pagination%22%3A%7B%7D%7D\"\n# read the html in the webpage\npg = read_html(sale)\n\n# get list of houses for sale that appears on the page\n# each property card is called an article when you inspect the webpage\nhousesale <- read_html(sale)%>%html_elements(\"article\")\n\n# create a dataframe for the FOR SALE houses\nres_pg <- tibble(\n  address= pg %>% html_nodes(xpath = \"/html/body/div[1]/div[5]/div/div/div[1]/div[1]/ul/li//div/div/article/div/div[1]/a/address\") %>% html_text(),\n  price = pg %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div/div/span') %>% html_text(),\n  bedrooms = pg %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[1]/b') %>% \n    html_text(),\n  bathrooms = pg %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[2]/b') %>% \n    html_text(),\n  sqft = pg %>% html_nodes(xpath = '/html/body/div[1]/div[5]/div/div/div/div[1]/ul/li//div/div/article/div/div[1]/div[3]/ul/li[3]/b') %>% \n    html_text()\n) \n\n# combine recently SOLD and FOR SALE houses in one data frame\nresults <- res_pg %>% bind_rows(res_ss)\nprint(results)\n\n# A tibble: 12 x 5\n   address                             price    bedrooms bathrooms sqft \n   <chr>                               <chr>    <chr>    <chr>     <chr>\n 1 201 10th Ave, Slater, IA 50244      $309,000 3        2         924  \n 2 50287 210th Hwy, Slater, IA 50244   $475,000 4        2         2,147\n 3 1013 Redbud Dr, Slater, IA 50244    $429,900 4        3         1,884\n 4 506 8th Ave, Slater, IA 50244       $328,000 4        3         1,180\n 5 604 Story St, Slater, IA 50244      $200,000 3        1         1,359\n 6 611 1st Ave N, Slater, IA 50244     $265,000 4        2         1,116\n 7 101 Main St, Slater, IA 50244       $255,000 5        3         1,896\n 8 1015 Redbud Dr, Slater, IA 50244    $397,732 2        3         1,325\n 9 107 Main St, Slater, IA 50244       $242,500 4        2         1,515\n10 52898 Highway 210, Slater, IA 50244 $400,000 4        1.75      2,550\n11 604 Marshall St, Slater, IA 50244   $135,000 3        1         1,166\n12 104 N Benton St, Slater, IA 50244   $210,000 3        2         1,056\n\n\nAt the beginning of the week, I helped my team assemble Excel spreadsheets with all the addresses for Slater, Grundy Center, Independence, and New Hampton. I was in charge of the Slater data and part of the Grundy Center and Independence data sets.\nWe needed to first gather the addresses for each city. Once they were gathered, we cleaned the data to remove any addresses that were obviously non-residential. We also narrowed the data down to just the Parcel ID and address. From there, we created the URLs to gather Google Street View images.\nWe used the following function in Excel to transform the street addresses into workable addresses for Google Street View:\n\n=TRIM(SUBSTITUTE(cell,CHAR(160),” “,”+“))\n\nThis function removes all spaces and replaces them with + signs.\nHere is the output of the cleaned Slater data:\n\n\n\n\n\nNext week, I will get around the anti-web scraping protections Beacon and Vanguard have on their sites. Beacon and Vanguard have information on houses that aren’t listed on Zillow, and more pictures aren’t listed on Zillow or Google Street View."
  }
]